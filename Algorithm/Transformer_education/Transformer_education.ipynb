{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_education.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "06tViAVBXov-"
      },
      "source": [
        "# education_train_text_segmentation\n",
        "import codecs\n",
        "import os\n",
        " \n",
        "file1 = open('drive/' + 'MyDrive/' + 'Corpus/' + 'corpus_education/' + 'education_en' + '.txt','w')\n",
        "file2 = open('drive/' + 'MyDrive/' + 'Corpus/' + 'corpus_education/' + 'education_cn' + '.txt','w')\n",
        "def separate_data(fpath):\n",
        "    content = codecs.open(fpath, 'r', 'utf-8').readlines()\n",
        "    for x in range(len(content[0:200000])):\n",
        "        if x % 2 == 0:\n",
        "            file1.write(content[x])\n",
        "        else:\n",
        "            file2.write(content[x])\n",
        " \n",
        "file_path = 'drive/MyDrive/Corpus/corpus_education/Bi-Education.txt'\n",
        "text1 = separate_data(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfiOUp8RZkd0"
      },
      "source": [
        "# education_test_text_segmentation\n",
        "import codecs\n",
        "import os\n",
        " \n",
        "file3 = open('drive/' + 'MyDrive/' + 'Corpus/' + 'corpus_education/' + 'education_en_test' + '.txt','w')\n",
        "file4 = open('drive/' + 'MyDrive/' + 'Corpus/' + 'corpus_education/' + 'education_cn_test' + '.txt','w')\n",
        "def separate_data(fpath):\n",
        "    content = codecs.open(fpath, 'r', 'utf-8').readlines()\n",
        "    for x in range(len(content)):\n",
        "        if x % 2 == 0:\n",
        "            file3.write(content[x])\n",
        "        else:\n",
        "            file4.write(content[x])\n",
        " \n",
        "file_path2 = 'drive/MyDrive/Corpus/corpus_education/education_test.txt'\n",
        "text2 = separate_data(file_path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fi0aewkSG8l",
        "outputId": "467e86d5-0bbf-4226-e146-b7bcfd6257cb"
      },
      "source": [
        "# word segmentation\n",
        " \n",
        "import jieba\n",
        "# Make Chinese word segmentation of sentences\n",
        "def seg_depart(sentence):\n",
        "    # Chinese word segmentation for each line in the document\n",
        "    # print(\"doing word segmentation\")\n",
        "    sentence_depart = jieba.cut(sentence.strip())\n",
        "    outstr = ''\n",
        "    for word in sentence_depart:\n",
        "        if word != '\\n':\n",
        "            outstr += word\n",
        "            outstr += \" \"\n",
        "    return outstr\n",
        " \n",
        "# Give the document path\n",
        "filename_train = \"drive/MyDrive/Corpus/corpus_education/education_cn.txt\"\n",
        "outfilename_train = \"drive/MyDrive/Corpus/corpus_education/education_out.txt\"\n",
        "inputs_train = open(filename_train, 'r', encoding='UTF-8')\n",
        "outputs_train = open(outfilename_train, 'w', encoding='UTF-8')\n",
        " \n",
        "filename_test = \"drive/MyDrive/Corpus/corpus_education/education_cn_test.txt\"\n",
        "outfilename_test = \"drive/MyDrive/Corpus/corpus_education/education_out_test.txt\"\n",
        "inputs_test = open(filename_test, 'r', encoding='UTF-8')\n",
        "outputs_test = open(outfilename_test, 'w', encoding='UTF-8')\n",
        " \n",
        "# Write the output to out.txt\n",
        "for line in inputs_test:\n",
        "    line_seg = seg_depart(line)\n",
        "    outputs_test.write(line_seg + '\\n')\n",
        "outputs_test.close()\n",
        "inputs_test.close()\n",
        "print(\"testing word segmentation successfully！！！\")\n",
        " \n",
        "for line in inputs_train:\n",
        "    line_seg = seg_depart(line)\n",
        "    outputs_train.write(line_seg + '\\n')\n",
        "outputs_train.close()\n",
        "inputs_train.close()\n",
        "print(\"training word segmentation successfully！！！\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing word segmentation successfully！！！\n",
            "training word segmentation successfully！！！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tTm68UuVKTJ"
      },
      "source": [
        "# hyperparams\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import codecs\n",
        "import regex\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# data\n",
        "source_train = 'drive/MyDrive/Corpus/corpus_education/education_out.txt'\n",
        "target_train = 'drive/MyDrive/Corpus/corpus_education/education_en.txt'\n",
        "source_test = 'drive/MyDrive/Corpus/corpus_education/education_out_test.txt'\n",
        "target_test = 'drive/MyDrive/Corpus/corpus_education/education_en_test.txt'\n",
        "\n",
        "# training\n",
        "batch_size = 32 # alias = N\n",
        "lr = 0.0001 # learning rate.\n",
        "logdir = 'drive/MyDrive/logdir_education' # log directory\n",
        "\n",
        "# model\n",
        "maxlen = 30 # Maximum number of words in a sentence. alias = T.\n",
        "min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
        "hidden_units = 512 # alias = C\n",
        "num_blocks = 6 # number of encoder/decoder blocks\n",
        "num_epochs = 20 # traversal time\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "sinusoid = False # select embedding method"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyF2vSqMUjKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe8c4e3-21d2-476b-8a32-2f8f563de1a9"
      },
      "source": [
        "# data preprocessing\n",
        "import codecs\n",
        "import os\n",
        "import regex\n",
        "from collections import Counter\n",
        "\n",
        "def make_vocab(fpath, fname):\n",
        "    text = codecs.open(fpath, 'r', 'utf-8').read()\n",
        "    # text = regex.sub(\"<[^>]+>\", \"\", text)\n",
        "    words = text.split()\n",
        "    word2cnt = Counter(words)\n",
        "    if not os.path.exists('drive/MyDrive/Corpus/corpus_education/preprocessed'): os.mkdir('drive/MyDrive/Corpus/corpus_education/preprocessed')\n",
        "    with codecs.open('drive/MyDrive/Corpus/corpus_education/preprocessed/{}'.format(fname), 'w', 'utf-8') as fout:\n",
        "        fout.write(\"{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n\".format(\"<PAD>\", \"<UNK>\", \"<S>\", \"</S>\"))\n",
        "        for word, cnt in word2cnt.most_common(len(word2cnt)):\n",
        "            fout.write(u\"{}\\t{}\\n\".format(word, cnt))\n",
        "\n",
        "make_vocab(source_train, \"cn.vocab.tsv\")\n",
        "make_vocab(target_train, \"en.vocab.tsv\")\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u48JGtemaAlL"
      },
      "source": [
        "def load_cn_vocab():\n",
        "    vocab = [line.split()[0] for line in codecs.open('drive/MyDrive/Corpus/corpus_education/preprocessed/cn.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=min_cnt]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def load_en_vocab():\n",
        "    vocab = [line.split()[0] for line in codecs.open('drive/MyDrive/Corpus/corpus_education/preprocessed/en.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=min_cnt]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def create_data(source_sents, target_sents): \n",
        "    cn2idx, idx2cn = load_cn_vocab()\n",
        "    en2idx, idx2en = load_en_vocab()\n",
        "    \n",
        "    # Index\n",
        "    x_list, y_list, Sources, Targets = [], [], [], []\n",
        "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
        "        x = [cn2idx.get(word, 1) for word in (source_sent + u\" </S>\").split()] # 1: OOV, </S>: End of Text\n",
        "        y = [en2idx.get(word, 1) for word in (target_sent + u\" </S>\").split()] \n",
        "        if max(len(x), len(y)) <=maxlen:\n",
        "            x_list.append(np.array(x))\n",
        "            y_list.append(np.array(y))\n",
        "            Sources.append(source_sent)\n",
        "            Targets.append(target_sent)\n",
        "    \n",
        "    # Pad      \n",
        "    X = np.zeros([len(x_list), maxlen], np.int32)\n",
        "    Y = np.zeros([len(y_list), maxlen], np.int32)\n",
        "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
        "        X[i] = np.lib.pad(x, [0, maxlen-len(x)], 'constant', constant_values=(0, 0))\n",
        "        Y[i] = np.lib.pad(y, [0, maxlen-len(y)], 'constant', constant_values=(0, 0))\n",
        "    \n",
        "    return X, Y, Sources, Targets\n",
        "\n",
        "def load_train_data():\n",
        "    de_sents = [regex.sub(\"<[^>]+>\", \"\", line) for line in codecs.open(source_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
        "    en_sents = [regex.sub(\"<[^>]+>\", \"\", line) for line in codecs.open(target_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
        "    \n",
        "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
        "    return X, Y\n",
        "    \n",
        "def load_test_data():\n",
        "    def _refine(line):\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        return line.strip()\n",
        "    \n",
        "    de_sents = [_refine(line) for line in codecs.open(source_test, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "    en_sents = [_refine(line) for line in codecs.open(target_test, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "        \n",
        "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
        "    return X, Sources, Targets # (1064, 150) \n",
        "\n",
        "def load_test_data1():\n",
        "    def _refine(line):\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        return line.strip()\n",
        "    \n",
        "    de_sents = [_refine(line) for line in codecs.open(source_test1, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "    en_sents = [_refine(line) for line in codecs.open(target_test1, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "        \n",
        "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
        "    return X, Sources, Targets # (1064, 150) \n",
        "\n",
        "def get_batch_data():\n",
        "    X, Y = load_train_data()\n",
        "    \n",
        "    # calculate batch size\n",
        "    num_batch = len(X) // batch_size\n",
        "    \n",
        "    X = tf.convert_to_tensor(X, tf.int32)\n",
        "    Y = tf.convert_to_tensor(Y, tf.int32)\n",
        "    \n",
        "    input_queues = tf.compat.v1.train.slice_input_producer([X, Y])\n",
        "            \n",
        "    x, y = tf.compat.v1.train.shuffle_batch(input_queues,\n",
        "                                num_threads=8,\n",
        "                                batch_size=batch_size, \n",
        "                                capacity=batch_size*64,   \n",
        "                                min_after_dequeue=batch_size*32, \n",
        "                                allow_smaller_final_batch=False)\n",
        "    \n",
        "    return x, y, num_batch # (N, T), (N, T), ()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBDYimw0Aww7"
      },
      "source": [
        "# layer normalization\n",
        "def normalize(inputs, epsilon = 1e-8, scope=\"ln\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        inputs_shape = inputs.get_shape()\n",
        "        params_shape = inputs_shape[-1:]\n",
        "    \n",
        "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "        beta= tf.Variable(tf.zeros(params_shape))\n",
        "        gamma = tf.Variable(tf.ones(params_shape))\n",
        "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
        "        outputs = gamma * normalized + beta\n",
        "        \n",
        "    return outputs\n",
        "\n",
        "# initial embedding operation\n",
        "def embedding(inputs, vocab_size, num_units, zero_pad=True, scale=True,scope=\"embedding\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        lookup_table = tf.get_variable('lookup_table',\n",
        "                                       dtype=tf.float32,\n",
        "                                       shape=[vocab_size, num_units],\n",
        "                                       initializer=tf.keras.initializers.glorot_normal()) \n",
        "        if zero_pad:\n",
        "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
        "                                      lookup_table[1:, :]), 0)\n",
        "        outputs = tf.nn.embedding_lookup(lookup_table, inputs) # complete word embedding, change into 3d tensor\n",
        "        \n",
        "        if scale:\n",
        "            outputs = outputs * (num_units ** 0.5) # scale\n",
        "            \n",
        "    return outputs\n",
        "    \n",
        "# positional encoding\n",
        "def positional_encoding(inputs, num_units,zero_pad=True, scale=True, scope=\"positional_encoding\", reuse=None):\n",
        "\n",
        "    N, T = inputs.get_shape().as_list()\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        \n",
        "        #position indices\n",
        "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n",
        "\n",
        "        position_enc = np.array([\n",
        "            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]\n",
        "            for pos in range(T)])\n",
        "\n",
        "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  \n",
        "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  \n",
        "\n",
        "        lookup_table = tf.convert_to_tensor(position_enc)\n",
        "\n",
        "        if zero_pad:\n",
        "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
        "                                      lookup_table[1:, :]), 0)\n",
        "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
        "\n",
        "        if scale:\n",
        "            outputs = outputs * num_units**0.5\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# attention mechanism\n",
        "def multihead_attention(queries, keys, num_units=None, num_heads=8, dropout_rate=0, is_training=True, causality=False, scope=\"multihead_attention\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Set the fall back option for num_units\n",
        "        if num_units is None:\n",
        "            num_units = queries.get_shape().as_list[-1]\n",
        "        \n",
        "        # Linear projections\n",
        "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
        "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        \n",
        "        # Split and concat\n",
        "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
        "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "\n",
        "        # Multiplication\n",
        "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
        "        \n",
        "        # Scale\n",
        "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "        \n",
        "        # Key Masking\n",
        "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
        "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
        "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
        "        \n",
        "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
        "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Causality = Future blinding\n",
        "        if causality:\n",
        "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
        "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
        "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
        "   \n",
        "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
        "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Activation\n",
        "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
        "         \n",
        "        # Query Masking\n",
        "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
        "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
        "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
        "        outputs *= query_masks # broadcasting. (h*N, T_q, T_k)\n",
        "          \n",
        "        # Dropouts avoid overfitting\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "               \n",
        "        # Weighted sum\n",
        "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
        "        \n",
        "        # Restore shape\n",
        "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
        "              \n",
        "        # Residual connection\n",
        "        outputs += queries\n",
        "              \n",
        "        # Normalize\n",
        "        outputs = normalize(outputs) # (N, T_q, C)\n",
        " \n",
        "    return outputs\n",
        "\n",
        "\n",
        "def feedforward(inputs, num_units=[2048, 512],scope=\"multihead_attention\", reuse=None):\n",
        "\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Inner layer\n",
        "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
        "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        \n",
        "        # Readout layer\n",
        "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
        "                  \"activation\": None, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        \n",
        "        # Residual connection\n",
        "        outputs += inputs\n",
        "        \n",
        "        # Normalize\n",
        "        outputs = normalize(outputs)\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "# smoothing\n",
        "def label_smoothing(inputs, epsilon=0.1):\n",
        "    ''' https://arxiv.org/abs/1512.00567.'''\n",
        "\n",
        "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
        "    return ((1-epsilon) * inputs) + (epsilon / K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwBZc64sA2j1",
        "outputId": "b4b68c9c-8dfd-42f6-e4d9-15d25c98b891"
      },
      "source": [
        "class Graph():\n",
        "    def __init__(self, is_training=True):\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n",
        "            else: # inference\n",
        "                self.x = tf.placeholder(tf.int32, shape=(None, maxlen))\n",
        "                self.y = tf.placeholder(tf.int32, shape=(None, maxlen))\n",
        "\n",
        "            # decoder input\n",
        "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
        "\n",
        "            # load   \n",
        "            de2idx, idx2de = load_cn_vocab()\n",
        "            en2idx, idx2en = load_en_vocab()\n",
        "            \n",
        "            # Encoder\n",
        "            with tf.variable_scope(\"encoder\"):\n",
        "                ## embedding\n",
        "                self.enc = embedding(self.x, \n",
        "                                      vocab_size=len(de2idx), \n",
        "                                      num_units=hidden_units, \n",
        "                                      scale=True,\n",
        "                                      scope=\"enc_embed\")\n",
        "                \n",
        "                ## positional embedding\n",
        "                if sinusoid:\n",
        "                    self.enc += positional_encoding(self.x,\n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"enc_pe\")\n",
        "                else:\n",
        "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"enc_pe\")\n",
        "                    \n",
        "                 \n",
        "                ## dropout layer\n",
        "                self.enc = tf.layers.dropout(self.enc, \n",
        "                                            rate=dropout_rate, \n",
        "                                            training=tf.convert_to_tensor(is_training))\n",
        "                \n",
        "                ## number of encoders: num_blocks\n",
        "                for i in range(num_blocks):\n",
        "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
        "                        ### Multihead Attention\n",
        "                        self.enc = multihead_attention(queries=self.enc, \n",
        "                                                        keys=self.enc, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads, \n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training,\n",
        "                                                        causality=False)\n",
        "                        \n",
        "                        ### Feed Forward\n",
        "                        self.enc = feedforward(self.enc, num_units=[4*hidden_units, hidden_units])\n",
        "            \n",
        "            # decoder\n",
        "            with tf.variable_scope(\"decoder\"):\n",
        "                self.dec = embedding(self.decoder_inputs, \n",
        "                                      vocab_size=len(en2idx), \n",
        "                                      num_units=hidden_units,\n",
        "                                      scale=True, \n",
        "                                      scope=\"dec_embed\")\n",
        "                \n",
        "                if sinusoid:\n",
        "                    self.dec += positional_encoding(self.decoder_inputs,\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"dec_pe\")\n",
        "                else:\n",
        "                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"dec_pe\")\n",
        "                \n",
        "                self.dec = tf.layers.dropout(self.dec, \n",
        "                                            rate=dropout_rate, \n",
        "                                            training=tf.convert_to_tensor(is_training))\n",
        "                \n",
        "                for i in range(num_blocks):\n",
        "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
        "                        ## Multihead Attention ( self-attention)\n",
        "                        self.dec = multihead_attention(queries=self.dec, \n",
        "                                                        keys=self.dec, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads, \n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training,\n",
        "                                                        causality=True, \n",
        "                                                        scope=\"self_attention\")\n",
        "                        \n",
        "                        ## Multihead Attention (vanilla attention)\n",
        "                        self.dec = multihead_attention(queries=self.dec, \n",
        "                                                        keys=self.enc, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads,\n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training, \n",
        "                                                        causality=False,\n",
        "                                                        scope=\"vanilla_attention\")\n",
        "                        \n",
        "                        ## Feed Forward\n",
        "                        self.dec = feedforward(self.dec, num_units=[4*hidden_units, hidden_units])\n",
        "                \n",
        "            # Final linear projection\n",
        "            self.logits = tf.layers.dense(self.dec, len(en2idx)) #shape: [N,T,len(en2idx)]\n",
        "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
        "            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
        "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
        "            tf.summary.scalar('acc', self.acc)\n",
        "            \n",
        "            if is_training:  \n",
        "                # smoothing\n",
        "                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n",
        "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
        "                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
        "               \n",
        "                # graph\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
        "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
        "                   \n",
        "                # summary\n",
        "                tf.summary.scalar('mean_loss', self.mean_loss)\n",
        "                self.merged = tf.summary.merge_all()\n",
        "\n",
        "if __name__ == '__main__':                    \n",
        "    de2idx, idx2de = load_cn_vocab()\n",
        "    en2idx, idx2en = load_en_vocab()\n",
        "    \n",
        "    g = Graph(\"train\"); print(\"load successfully\")\n",
        "    \n",
        "    sv = tf.train.Supervisor(graph=g.graph, \n",
        "                             logdir=logdir,\n",
        "                             save_model_secs=0)\n",
        "    with sv.managed_session() as sess:\n",
        "        for epoch in range(1, num_epochs+1): \n",
        "            print('The%d' % (epoch)+'period')\n",
        "            if sv.should_stop(): break\n",
        "            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
        "                sess.run(g.train_op)\n",
        "                \n",
        "            gs = sess.run(g.global_step)   \n",
        "            sv.saver.save(sess, logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
        "    \n",
        "    print(\"Done\")    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-1cad60bb117a>:77: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-5-1cad60bb117a>:84: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:202: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  warnings.warn('`tf.layers.conv1d` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-fe13391312c1>:117: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-7-fe13391312c1>:118: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "load successfully\n",
            "WARNING:tensorflow:From <ipython-input-7-fe13391312c1>:145: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The1period\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Recording summary at step 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 28%|████████▌                      | 581/2096 [01:59<03:49,  6.61b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 582.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 28%|████████▋                      | 584/2096 [02:00<04:03,  6.22b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 4.98549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 66%|███████████████████▊          | 1382/2096 [04:00<01:52,  6.32b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 1382.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 66%|███████████████████▊          | 1385/2096 [04:00<01:55,  6.15b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.67685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The2period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▉                               | 60/2096 [00:09<05:24,  6.28b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2156.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▉                               | 63/2096 [00:09<05:38,  6.00b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.44999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 41%|████████████▊                  | 863/2096 [02:09<03:11,  6.44b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 2959.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 41%|████████████▊                  | 866/2096 [02:09<03:14,  6.32b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.69145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 79%|███████████████████████▊      | 1666/2096 [04:09<01:05,  6.58b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 3762.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████████████████████▉      | 1669/2096 [04:09<01:07,  6.34b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.69185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The3period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█████▏                         | 347/2096 [00:52<04:20,  6.70b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 4539.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█████▏                         | 350/2096 [00:52<04:40,  6.23b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 55%|████████████████▍             | 1152/2096 [02:52<02:18,  6.84b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 5344.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 55%|████████████████▌             | 1155/2096 [02:52<02:29,  6.29b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.70816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|███████████████████████████▉  | 1956/2096 [04:52<00:22,  6.15b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 6148.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|████████████████████████████  | 1958/2096 [04:52<00:21,  6.43b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.68986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The4period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|█████████▍                     | 635/2096 [01:35<03:37,  6.72b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 6923.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|█████████▍                     | 638/2096 [01:35<03:45,  6.48b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 6.4681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|████████████████████▋         | 1441/2096 [03:35<01:38,  6.62b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 7729.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The5period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|█▊                             | 124/2096 [00:19<05:28,  6.01b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 8508.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|█████████████▋                 | 928/2096 [02:19<03:10,  6.12b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 9312.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83%|████████████████████████▊     | 1733/2096 [04:19<00:54,  6.69b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 10117.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The6period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██████▏                        | 418/2096 [01:02<04:33,  6.13b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 10898.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████████████████▌            | 1223/2096 [03:02<02:09,  6.72b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 11703.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 97%|█████████████████████████████ | 2030/2096 [05:02<00:10,  6.17b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 12510.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The7period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 34%|██████████▌                    | 711/2096 [01:46<03:45,  6.14b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 13287.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|█████████████████████▋        | 1517/2096 [03:46<01:35,  6.09b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 14093.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The8period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|██▉                            | 199/2096 [00:30<04:54,  6.45b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 14871.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|██████████████▎               | 1004/2096 [02:30<02:53,  6.29b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 15676.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|█████████████████████████▉    | 1809/2096 [04:30<00:43,  6.64b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 16481.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The9period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|███████▏                       | 488/2096 [01:13<03:58,  6.75b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 17256.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 62%|██████████████████▌           | 1295/2096 [03:13<02:05,  6.37b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 18063.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 18864.\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The10period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███████████▋                   | 786/2096 [01:57<03:20,  6.54b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 19650.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 76%|██████████████████████▊       | 1593/2096 [03:57<01:16,  6.60b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 20457.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The11period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|████                           | 273/2096 [00:41<04:32,  6.70b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 21233.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 51%|███████████████▍              | 1079/2096 [02:41<02:45,  6.15b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 22039.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|██████████████████████████▉   | 1886/2096 [04:41<00:31,  6.70b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 22846.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The12period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 27%|████████▍                      | 570/2096 [01:25<04:12,  6.04b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 23626.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 65%|███████████████████▌          | 1369/2096 [03:25<01:48,  6.70b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 24425.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The13period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▊                               | 56/2096 [00:08<05:42,  5.96b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 25208.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 41%|████████████▋                  | 860/2096 [02:08<03:03,  6.72b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 26012.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████████████████████▊      | 1667/2096 [04:08<01:10,  6.08b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 26819.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The14period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█████▏                         | 351/2096 [00:52<04:39,  6.25b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 27599.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 55%|████████████████▌             | 1156/2096 [02:52<02:22,  6.58b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 28405.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 94%|████████████████████████████  | 1962/2096 [04:52<00:19,  6.73b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 29210.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The15period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|█████████▌                     | 647/2096 [01:36<03:42,  6.52b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 29991.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 69%|████████████████████▊         | 1453/2096 [03:36<01:34,  6.81b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 30797.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The16period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|█▉                             | 133/2096 [00:20<04:57,  6.60b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 31573.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|█████████████▊                 | 938/2096 [02:20<02:58,  6.47b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 32378.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83%|████████████████████████▉     | 1743/2096 [04:20<00:53,  6.62b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 33183.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The17period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██████▎                        | 429/2096 [01:04<04:10,  6.67b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 33965.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 59%|█████████████████▋            | 1235/2096 [03:04<02:23,  6.01b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 34771.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 97%|█████████████████████████████▏| 2042/2096 [05:04<00:08,  6.54b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 35578.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The18period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 34%|██████████▋                    | 722/2096 [01:48<03:26,  6.66b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 36354.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|█████████████████████▊        | 1527/2096 [03:48<01:28,  6.43b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 37159.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The19period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|███                            | 209/2096 [00:31<05:12,  6.03b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 37937.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|██████████████▍               | 1013/2096 [02:31<02:55,  6.17b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 38741.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 87%|██████████████████████████    | 1818/2096 [04:31<00:40,  6.81b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 39546.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                         | 0/2096 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The20period\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 24%|███████▎                       | 498/2096 [01:14<03:56,  6.77b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Recording summary at step 40322.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|███████████▎                   | 761/2096 [01:53<03:17,  6.75b/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cUyOWQdk7gw",
        "outputId": "a59c681c-77ca-4588-f18d-4bb74525b255"
      },
      "source": [
        "def eval(): \n",
        "    # load graph\n",
        "    g = Graph(is_training=False)\n",
        "    print(\"Graph loaded\")\n",
        "    \n",
        "    # load data\n",
        "    X, Sources, Targets = load_test_data()\n",
        "    cn2idx, idx2cn = load_cn_vocab()\n",
        "    en2idx, idx2en = load_en_vocab()\n",
        "     \n",
        "     \n",
        "    # Start session \n",
        "    with g.graph.as_default():    \n",
        "        sv = tf.train.Supervisor()\n",
        "        with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "            ## restore parameters\n",
        "            sv.saver.restore(sess, tf.train.latest_checkpoint(logdir))\n",
        "            print(\"Restored!\")\n",
        "              \n",
        "            ## Get model name\n",
        "            mname = open(logdir + '/checkpoint', 'r').read().split('\"')[1] # model name\n",
        "             \n",
        "            ## Inference\n",
        "            if not os.path.exists('drive/Shareddrives/ruochen.katherina@gmail.com/results_education'): os.mkdir('drive/Shareddrives/ruochen.katherina@gmail.com/results_education')\n",
        "            with codecs.open(\"drive/Shareddrives/ruochen.katherina@gmail.com/results_education/\" + mname, \"w\", \"utf-8\") as fout:\n",
        "                list_of_refs, hypotheses = [], []\n",
        "                for i in range(len(X) // batch_size):\n",
        "                     \n",
        "                    ### Get mini-batches\n",
        "                    x = X[i*batch_size: (i+1)*batch_size]\n",
        "                    sources = Sources[i*batch_size: (i+1)*batch_size]\n",
        "                    targets = Targets[i*batch_size: (i+1)*batch_size]\n",
        "                     \n",
        "                    ### Autoregressive inference\n",
        "                    preds = np.zeros((batch_size, maxlen), np.int32)\n",
        "                    for j in range(maxlen):\n",
        "                        _preds = sess.run(g.preds, {g.x: x, g.y: preds})\n",
        "                        preds[:, j] = _preds[:, j]\n",
        "                     \n",
        "                    ### Write to file\n",
        "                    for source, target, pred in zip(sources, targets, preds): # sentence-wise\n",
        "                        got = \" \".join(idx2en[idx] for idx in pred).split(\"</S>\")[0].strip()\n",
        "                        fout.write(\"- source: \" + source +\"\\n\")\n",
        "                        fout.write(\"- expected: \" + target + \"\\n\")\n",
        "                        fout.write(\"- got: \" + got + \"\\n\\n\")\n",
        "                        print(\"- source: \" + source +\"\\n\")\n",
        "                        print(\"- expected: \" + target + \"\\n\")\n",
        "                        print(\"- got: \" + got + \"\\n\\n\")\n",
        "                        fout.flush()\n",
        "                          \n",
        "                        #bleu score\n",
        "                        ref = target.split()\n",
        "                        hypothesis = got.split()\n",
        "                        if len(ref) > 3 and len(hypothesis) > 3:\n",
        "                            list_of_refs.append([ref])\n",
        "                            hypotheses.append(hypothesis)\n",
        "              \n",
        "                ## Calculate bleu score\n",
        "                print(list_of_refs)\n",
        "                score = corpus_bleu(list_of_refs, hypotheses)\n",
        "                fout.write(\"Bleu Score = \" + str(100*score))\n",
        "                print(\"Bleu Score = \" + str(100*score))\n",
        "                                          \n",
        "if __name__ == '__main__':\n",
        "    eval()\n",
        "    print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:202: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  warnings.warn('`tf.layers.conv1d` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph loaded\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Restoring parameters from drive/MyDrive/logdir_education/model_epoch_19_gs_39824\n",
            "Restored!\n",
            "- source: 这位 老 教授 是 在 1949 年 回到 中国 的 ， 当时 大陆 刚 解放 。\n",
            "\n",
            "- expected: The old prefessor came back to Chine in 1949 when the mainland had just been liberated.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 全 社会 都 应 关心 孩子 的 健康成长 。\n",
            "\n",
            "- expected: The whole society should be concerned about the health and sound growth of children.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> are all full of <UNK>\n",
            "\n",
            "\n",
            "- source: 他们 为 自己 的 祖国 献出 了 生命 ， 被誉为 英雄 。\n",
            "\n",
            "- expected: They gave their lives for their country and were honoured as heroes.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> from <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 为了 不 留下 指纹 ， 他们 一定 戴上 了 手套 。\n",
            "\n",
            "- expected: They must have worn gloves in order not to leave any fingerprints.\n",
            "\n",
            "- got: <UNK> I think I'll go and knock around in <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 那年 饥荒 饿死 了 几千人 。\n",
            "\n",
            "- expected: Thousands of people were starved to death during the famine year.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 为了 进入 大学 你 必须 通过 一系列 考试 。\n",
            "\n",
            "- expected: To get into university you have to pass a number of examinations.\n",
            "\n",
            "- got: <UNK> <UNK> can be a <UNK> but you have to be a good <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 以 每 小时 一哩 的 速度 行进 。\n",
            "\n",
            "- expected: We are walking at the speed of one mile at hour.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 邀请 他来 这儿 讲演 ， 所以 我们 最好 去 听讲 。\n",
            "\n",
            "- expected: We have invite him here to speak, so we'd better go to his lecture.\n",
            "\n",
            "- got: <UNK> I go out with an <UNK> <UNK> <UNK> <UNK> and find the <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 宁愿 住 郊区 而 不愿 住 城里 。\n",
            "\n",
            "- expected: We would rather live in the suburbs than in the city.\n",
            "\n",
            "- got: <UNK> <UNK> I go up to the <UNK> <UNK> <UNK> and I see <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 看 一个 人 ， 不是 根据 他 的 表白 ， 而是 根据 他 的 行动 。\n",
            "\n",
            "- expected: We judge a person not by what he says but by what he does.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> in their <UNK> but they all have their <UNK>\n",
            "\n",
            "\n",
            "- source: 判断 一个 人应 根据 他 的 行动 ， 而 不是 他 的 言词 。\n",
            "\n",
            "- expected: A man should be judged by his deeds, not his words.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> are all clear of this <UNK>\n",
            "\n",
            "\n",
            "- source: 你 如果 能 正确 和 有效 地 使用 一个 字 ， 你 就 了解 它 了 。\n",
            "\n",
            "- expected: If you can use a word correctly and effectively, you comprehend it.\n",
            "\n",
            "- got: If you have a <UNK> database administrator may have to have the same ability to do it.\n",
            "\n",
            "\n",
            "- source: 他 在 第一轮 比赛 中 猛击 一拳 就 把 对手 打倒 在 地 。\n",
            "\n",
            "- expected: He floored his opponent with a fine punch in the first round.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> are all around him.\n",
            "\n",
            "\n",
            "- source: 他们 向 学院 赠送 了 一笔 款项 以 纪念 他们 的 儿子 。\n",
            "\n",
            "- expected: They presented a sum of money to the college in memory of their son.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> are all around him.\n",
            "\n",
            "\n",
            "- source: 关于 你 的 秘密 ， 我 决不会 向 别人 泄露 一个 字 的 。\n",
            "\n",
            "- expected: I will not breathe a word of your secret to another man.\n",
            "\n",
            "- got: <UNK> I know you will not be a <UNK> or a <UNK> but I know I will always be able to do that.\n",
            "\n",
            "\n",
            "- source: 我 肯定 人们 会 认为 ， 啊 ， 那 是因为 我 跟 蒂姆 上床 。\n",
            "\n",
            "- expected: I'm sure people will think, Aah, it's because I've slept with Tim.\n",
            "\n",
            "- got: <UNK> I go out with an <UNK> <UNK> <UNK> <UNK> and find the <UNK>\n",
            "\n",
            "\n",
            "- source: 在 标准 的 英国 口音 中 ， a （ 比如 在 father 中 ） 被 发为 aah ， 而 不 像 Apple 中 的 a 。\n",
            "\n",
            "- expected: The standard English accent, the a (for example in father) is pronounced aah, not like a like apple.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 当 电力公司 打开 球上 的 数千只 小 灯泡 时 ， 人们 口中 喊 着 “ 嚯 ， 啊 ” 。\n",
            "\n",
            "- expected: They say \"ooh\" and \"aah\" when the electric company turns on the thousands of little lights in the ball.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 目的 研究 前列腺 非典型 腺瘤 性 增生 （ AAH ） 的 诊断 和 鉴别 诊断 。\n",
            "\n",
            "- expected: Objective Diagnosis and differential diagnosis of atypical adenomatous hyperplasia of the prostate (AAH) were studied.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 已经 提出 肺 的 不 典型 腺瘤 样 增生 （ AAH ） 到 支气管 肺泡 癌 （ BAC ） 再 到 侵袭 性肺 腺癌 发病 过程 。\n",
            "\n",
            "- expected: It has been proposed that stepwise progression occurs from atypical adenomatous hyperplasia (AAH) through bronchioloalveolar carcinoma (BAC) to invasive lung adenocarcinoma.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 近来 认为 人 肺部 非典型 腺瘤 性 增生 （ AAH ） 可能 是 细 支气管 肺泡 癌 （ BAC ） 的 前期 病变 。\n",
            "\n",
            "- expected: Atypical Adenomatous Hyperplasia (AAH) of the human lung has been recently implicated as a possible precursor lesion of bronchioloalveolar carcinoma (BAC).\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 另外 ， 提出 一种 改进 的 直方图 — — 平均 面积 直方图 ， 以 提取 色彩 特征 。\n",
            "\n",
            "- expected: Moreover, an improvement on the traditional histogram called the average area histogram (AAH) is proposed to represent color features.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 当球 上 的 数千 盏 小 灯亮 起时 ， 人群 一阵 欢呼雀跃 。\n",
            "\n",
            "- expected: They ooh and aah when the thousands of little lights in the ball come on.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: AAH 在 免疫组化 ， 形态学 ， 流式细胞 计数 ， 基因 异常 方面 与 腺癌 有 共同点 。\n",
            "\n",
            "- expected: AAH has been shown to have immunohistochemical, morphometric, flow cytometric and genetic abnormalities overlapping with adenocarcinoma.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 啊 ， 这 还 不错 ， 沃 尔登 想 ， 这 一点 我们 也许 能够 办到 。\n",
            "\n",
            "- expected: Aah, that's not so bad, Walden thought. That we might be able to manage.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 球上 成千上万 的 小灯 开始 闪烁 的 那一刻 ， 人们 全都 欢呼 。\n",
            "\n",
            "- expected: They ooh and aah when the thousands of little lights in the ball come on.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 车上 装备 有 霰弹枪 、 催泪弹 、 一部 夜视 摄像机 和 几包 和 奥巴马 同型 的 AB型 血 。\n",
            "\n",
            "- expected: It is equipped with shotguns, tear gas, a night- vision camera and bags of Obama's blood (group AB).\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我 的 语法 当其 条件 允许 时 找到 了 一个 ABC ， 但是 当 只能 找到 A 或者 AB   时 ， 也 能 满足 于 此 。\n",
            "\n",
            "- expected: My grammar finds an ABC when it is available, but settles for an A, or AB, when that is all there is to find.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 注意 ， 在 解析器 寻找 末尾 的 单词 之前 ， 会 尝试 （ ABC | AB | A ） 系列 中 的 全部 可选项 。\n",
            "\n",
            "- expected: Notice that alternatives within the (ABC | AB | A) alternation are all tried before the parser looks for the trailing words.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 搜索 一个 3 字母 的 单词 ， 比如 ABC ， 可以 通过 搜索 AB 和 BC 来 实现 。\n",
            "\n",
            "- expected: A search for a 3 - letter word, like ABC, can be done by searching for AB and BC.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 现在 我们 可以 回答 最初 的 问题 了 ， 我们 把 OA ， AB 和 BP 加到 一起 。\n",
            "\n",
            "- expected: So, now we can answer the initial question because vector OP, well, we just add up OA, AB, and BP.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 接下来 ， 在 根 用户 的 终端 上 列出   / tmp 的 内容 清单 ； 注意 ， 这里 有 其他 文件 ， 但是 没有   / tmp / ab 。\n",
            "\n",
            "- expected: Next, get a listing of the /tmp contents in the root terminal; notice that there are likely other files but no /tmp/ab.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: ab 工具 易于 使用 ： 只 需 提供 给 它 一个 重复 数 和 一个 URL 。\n",
            "\n",
            "- expected: The ab utility is simple to use: Provide a repeat count and a URL.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: ab 用于 为 PHP 页面 自动化 处理 大量 的 请求 。\n",
            "\n",
            "- expected: Use ab to automate a large number of requests for PHP pages.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: ab 工具 对 这个 URL   提交 若干次 请求 并 返回 统计 信息 。\n",
            "\n",
            "- expected: The ab utility requests that URL many times over and returns statistics.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 这次 新 展览 展出 的 一些 作品 可能 会 让 前来 观展 的 骨灰级 科幻 迷们 大吃一惊 。\n",
            "\n",
            "- expected: Descending into the new exhibition, hard-core science fiction fans may be taken aback at some of the work around them.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 但是 ， 公众 对 他 的 困境 的 同情 可能 会令 他 感到 迷惑 。\n",
            "\n",
            "- expected: But he might be taken aback by the popular sympathy for his plight.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 在 我 最初 被 这群 消极 人类 弄 得 措手不及 的 同时 ， 我 终于 学会 了 处理 这 事儿 并且 把 它 引导 成 自觉 行为 。\n",
            "\n",
            "- expected: While I was initially taken aback by negativity of the people, I eventually learned to manage it and channel it into conscious action.\n",
            "\n",
            "- got: <UNK> <UNK> I can <UNK> but I need to put it in the middle of my <UNK>\n",
            "\n",
            "\n",
            "- source: 在 这点 上 我们 被 震惊 了 ， 被 这种 粗鲁 的 打断 深深 震惊 了 , 否则 诗人 的 悲叹 将会 天衣无缝 的 流畅 通顺 下去 。\n",
            "\n",
            "- expected: I think we're taken aback here. We're taken aback by the rude interruption in the otherwise seamless flow of the poet's lament.\n",
            "\n",
            "- got: <UNK> <UNK> I can <UNK> but I have to say it by ear.\n",
            "\n",
            "\n",
            "- source: 许多 在 瓦拉 旅途 中 与 他 相遇 的 撒 玛利亚 人 、 记者 以及 其他人 都 对 瓦拉 先生 的 容貌 感到 惊诧 。\n",
            "\n",
            "- expected: Many Samaritans who met him on his travels, as well as journalists and others, were rather taken aback by Mr Varah in person.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> by the <UNK> of <UNK>\n",
            "\n",
            "\n",
            "- source: 但是 就 连 他 也 可能 会 对 彼得 · 费尔罗 调制 的 配方 大吃一惊 。\n",
            "\n",
            "- expected: But even he might have been taken aback by the recipe concocted by Peter Ferlow.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> are both the <UNK> of <UNK>\n",
            "\n",
            "\n",
            "- source: 在 你 第一次 展卷 阅读 时 ， 不要 惊讶 于 那些 困扰 你 的 问题 。\n",
            "\n",
            "- expected: You should not be taken aback with some issues that might put you off at first reading.\n",
            "\n",
            "- got: <UNK> I go out with an <UNK> <UNK> <UNK> <UNK> and find the <UNK>\n",
            "\n",
            "\n",
            "- source: 乔纳森 说 ， 吃 了 一惊 ， 但 又 感到 骄傲 ， 因为 长者 已经 注意 到 他 。\n",
            "\n",
            "- expected: I enjoy speed, ” Jonathan said, taken aback but proud that the Elder had noticed.\n",
            "\n",
            "- got: <UNK> <UNK> I will have to get <UNK> to <UNK> and make the <UNK>\n",
            "\n",
            "\n",
            "- source: 我 和 小组 中 其他 几位 职 场上 的 年轻人 都 被 他 不经意 的 回答 吓了一跳 。\n",
            "\n",
            "- expected: I, and several of the other young professionals in our group, were taken aback by his casual remark.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 你们 中 很多 人 认为 这种 把 责任 交还给 人们 自己 的 方式 意味着 放弃 他们 或者 让 他们 完全 靠 自己 解决问题 。\n",
            "\n",
            "- expected: Many of you feel that returning responsibility to people in this way means to abandon them or to tell them to solve the issue all by themselves.\n",
            "\n",
            "- got: <UNK> I think it would be good if we were to get over with these <UNK>\n",
            "\n",
            "\n",
            "- source: 放手 ： 我 认为 受过 教育 的 人 觉得 难以 接受 ， 不能 放弃 他们 自己 和 他们 的 想法 ， 不会 放手 他们 正在 控制 的 事物 。\n",
            "\n",
            "- expected: Letting Go: I think educated people find it harder to just accept, to abandon themselves and their ideas, to let go of what control they have.\n",
            "\n",
            "- got: <UNK> I go out with them, but I hope <UNK> and tell them to come off my <UNK>\n",
            "\n",
            "\n",
            "- source: 你们 中 很多 人 以为 这种 把 责任 交还给 人们 自己 的 方式 意味着 放弃 他们 或者 让 他们 完全 靠 自己 解决 题目 。\n",
            "\n",
            "- expected: Many of you feel that returning responsibility to people in this way means to abandon them or to tell them to solve the issue all by themselves.\n",
            "\n",
            "- got: <UNK> <UNK> I have to get <UNK> I have to know of it.\n",
            "\n",
            "\n",
            "- source: 有些 对 加密 持 批评 态度 的 人 认为 遵纪守法 的 公民 无须 隐瞒 什么 ， 但 这种 看法 并 不 正确 。\n",
            "\n",
            "- expected: Those critical of encryption might suggest that law- abiding citizens have nothing to hide, but that simply isn't true.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> <UNK> and are the <UNK> of <UNK>\n",
            "\n",
            "\n",
            "- source: 在 这件 事上 \" OK 丘比特 \" 有 不变 的 信仰 ， 这 就是 数学 。\n",
            "\n",
            "- expected: To the extent that OK Cupid has any abiding faith, it is in mathematics.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 所有 遵守 耶稣 的 爱 意思 是 继续 相信 ， 每时每刻 ， 我们 都 是 被 关爱 着 的 。\n",
            "\n",
            "- expected: So abiding in his love means continuing to believe, moment by moment, that we are loved.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> in the same way as the <UNK>\n",
            "\n",
            "\n",
            "- source: 在 创建 一个 新 项目 之后 ， 我们 就 得到 了 一个 类似 于图 2   所示 的 目录 结构 。\n",
            "\n",
            "- expected: After you create a project, you wind up with a directory structure similar to the one illustrated in Figure 2.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 那么 ， 这些 值 如何 出现 在 微调 器中 ， 应用程序 如何 在 英制 单位 与 公制 单位 之间 切换 ？\n",
            "\n",
            "- expected: How do these actually wind up in the spinner, and how can the application toggle between English and metric units?\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 现在 的 问题 是 ， 上述 哪家 财力雄厚 的 集团 — — 如果 有 的话 — — 最终 将 与 《 赫芬顿 邮报 》 合作 。\n",
            "\n",
            "- expected: The question now is which of the deep pocketed groups listed above — if any — will wind up partnering with Huffington.\n",
            "\n",
            "- got: <UNK> <UNK> can be a <UNK> but it does take a look at the beginning of the <UNK>\n",
            "\n",
            "\n",
            "- source: 所以 ， 那些 年轻人 的 单身 派对 ， 喝 了 一夜 的 酒 之后 ， 最后 和 陌生人 睡 到 一起 这种 事 也 就 不足为奇 了 。\n",
            "\n",
            "- expected: Its no wonder, then, that so many young, single partiers wind up sleeping with new people after a long night of guzzling beer.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 人们 以为 他会 轻易 获胜 ， 但是 他 却 被 打 得 一败涂地 。\n",
            "\n",
            "- expected: He was expected to win in a walk, but he was badly beaten.\n",
            "\n",
            "- got: <UNK> I go up to the <UNK> <UNK> ” <UNK> said <UNK>\n",
            "\n",
            "\n",
            "- source: 每天 几十分钟 的 步行 对于 现代人 来说 是 一举 双得 的 活动 。 它 不仅 有助于 你 消化 ， 也 节约 你 的 锻炼 时间 。\n",
            "\n",
            "- expected: Walk can be a win -win excise in your daily life, which will assist ur digestion and save ur Exercise time.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> and <UNK> have all been <UNK> over the past three years, and we have nothing to do with <UNK>\n",
            "\n",
            "\n",
            "- source: 现在 还有 一场 球 ， 希望 热刺 下周 会 有所 闪失 ， 同时 我们 获胜 进军 下 赛季 欧罗巴 。\n",
            "\n",
            "- expected: There's still one game to go and hopefully they (Spurs) will draw next week and we win and get into Europe next season.\n",
            "\n",
            "- got: <UNK> I can go up to the lodge for some hot dogs and <UNK>\n",
            "\n",
            "\n",
            "- source: 最终 ， 事态 的 发展 清楚 地 表明 ， 唯一 能够 赢得 选举 的 稳妥 途径 是 ： 致 对手 于 死地 。\n",
            "\n",
            "- expected: Ultimately, it becomes clear that there's only one sure way to win the election: kill the other guy. And the battle begins.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 没有 人 料到 会 是 这个 结果 — — 这样 一家 债务缠身 、 现金 短缺 的 意大利 公司 ， 是 无法 在 国际法庭 上 打赢 这场 官司 的 。\n",
            "\n",
            "- expected: No one thought it could happen -- there's no way this debt -laden, cash strapped Italian company will win in an international court of law.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 以 他 的 踢法 ， 他 绝对 是 热门 质疑 ， 如果 今年 他 无法 赢得 这个 奖 ， 那么 他 肯定 也 会 在 未来 赢得 。\n",
            "\n",
            "- expected: The way he's going, he is one of the favourites and if he doesn't win it this year, he'll certainly win it at some point.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> in the same <UNK> as the one who wants to see or the <UNK>\n",
            "\n",
            "\n",
            "- source: 终于 有 一次 ， 伯恩斯 深陷 在 华盛顿 的 政治 中 ， 以至于 他 再也 顾不上 罗斯 巴德 了 。\n",
            "\n",
            "- expected: Only once Burns became so wrapped up in Washington politics that he could no longer care did Rothbard finally win out.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and so on.\n",
            "\n",
            "\n",
            "- source: 哪种 新 保守主义 的 潮流 会 胜出 ？   是 对 民主 的 追求 ， 抑或 只是 对 伊斯兰 运动 夺取 政权 的 恐惧 ？\n",
            "\n",
            "- expected: Which neoconservative impulse will win out -- the embrace of democratic longing, or the fear of Islamic movements taking power?\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 要 让 孩子 们 对 书本 产生 渴求 ， 否则 电视 总会 赢 。\n",
            "\n",
            "- expected: We have to create in children a deep seated need for books, otherwise television will always win out.\n",
            "\n",
            "- got: <UNK> I can go up to the lodge for some hot dogs and <UNK>\n",
            "\n",
            "\n",
            "- source: “ 当前 通胀 压力 较大 ， 中央政治局 主要 关注 通胀 水平 。 ” 辉说 ， “ 因此 期望 汇率 更富 弹性 的 一方 胜出 。 ”\n",
            "\n",
            "- expected: “At times when there is more inflation pressure, the politburo is primarily concerned about inflation ,” Hui says.“ That's when those arguing for more flexibility tend to win out.”\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 如果 继续 让 混乱 与 嫉妒 混淆 人们 的 理智 ， 核扩散 非但 不会 得到 限制 ， 反而 会 成为 一件 让 所有人 后悔 的 憾事 。\n",
            "\n",
            "- expected: But if muddle and jealousy win out, nuclear proliferation, not restraint, will be the norm—to enduring regret all round.\n",
            "\n",
            "- got: <UNK> <UNK> I have to get <UNK> <UNK> <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 凭直觉 判断 ， 经受 了 实际 需求 的 考验 要 胜过 理论家 的 空想 ， 并且 我 认为 这种 直觉 完全 经得起 更进一步 的 检验 。\n",
            "\n",
            "- expected: It seems rather intuitive that crucibles of burning need win out over eggheads, and I suppose that intuition stands up well to closer inspection.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 小 碎步 似的 恢复 带来 了 以下 问题 ： 日本 能否 克服 悲剧 的 后遗症 ？\n",
            "\n",
            "- expected: The slow pace of recovery raises the question: Which Japan will win out in the aftermath of the tragedy?\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 以 法律 的 力量 保护 言论自由 ， 我们 借助 理性 的 力量 战胜 仇恨 。\n",
            "\n",
            "- expected: We protect free speech with the force of law, and we appeal to the force of reason to win out over hate.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and so on.\n",
            "\n",
            "\n",
            "- source: 我 始终 想着 老爸 在 临终前 确信 的 预言 ， 他 说 我会 胜出 。\n",
            "\n",
            "- expected: And I couldn ’t get Daddy and his confident deathbed prediction that I would win out of my mind.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 但 要 期待 市场 力量 压倒 政治 忧虑 ， 只有 寄望 于 奇迹 。\n",
            "\n",
            "- expected: But it will take a miracle for market forces to win out over political fear.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> together to create one of the world's most <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 非正式 的 调查 和 访谈 都 显示 素食 能 改善 体味 。\n",
            "\n",
            "- expected: Informal polls and interviews generally result in the concession that when it comes to body odors, vegetarians usually win out.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 无论 需要 多长时间 去 击败 这次 预谋 的 侵略 ， 美国 人民 正义 在手 ， 有 力量 夺取 彻底 的 胜利 。\n",
            "\n",
            "- expected: No matter how long it may take us to overcome this premeditated invasion, the American people in their righteous might will win through to absolute victory.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 众多 选手 要 在 如此 激烈 的 竞争 中 胜出 并非易事 。\n",
            "\n",
            "- expected: It’s not an easy task for the contestants to win through the fierce competition.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> or <UNK>\n",
            "\n",
            "\n",
            "- source: 那 将 是 艰巨 的 工作 ， 不过 历经 千辛万苦 后 我们 最后 会 取得胜利 的 。 答案 。\n",
            "\n",
            "- expected: It's going to be hard work but we'll win through in the end.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 他 面对 不少 风浪 ， 但 他 都 能 藉 他 的 政治 智能 渡过难关 。 假如 是 其它 政客 ， 可能 早就 面临 失败 。\n",
            "\n",
            "- expected: He has weathered many political storms, using his political wit to win through whee other politician would have faltered.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> are both the <UNK> of <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 在 英文 学习 中 ， 千万 不要 回避 自己 的 短处 ， 唯有 正视 它们 ， 你 才能 终获 成功 。\n",
            "\n",
            "- expected: Don't try to slur over your weaknesses in English learning; only by facing up to them, can you win through in the end.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 事在人为 ，   努力 干 下去 ， 总会 赢得 胜利 的 。\n",
            "\n",
            "- expected: Nothing's impossible and it all depends on man; you would win through if you persist.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> are all way to <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 除了 这些 治疗 音乐 之外 ， 音乐 治疗师 还会 经常 挑选 一些 流行歌曲 放入 音疗库 ， 如 “ 爱 拼 才会赢 ” 等 。\n",
            "\n",
            "- expected: \"In addition to music therapy, music therapists will often pop into the selection of some of the sound library treatment, such as\" eager to win through struggle.\n",
            "\n",
            "- got: <UNK> <UNK> I can <UNK> but I have to say it <UNK>\n",
            "\n",
            "\n",
            "- source: 2008 年 博鳌 论坛 年会 将 于 星期五 持续 到 星期天 ， 围绕 “ 绿色 亚洲 ： 通过 变化 走向 双赢 ” 这样 一个 主题 展开 。\n",
            "\n",
            "- expected: The 2008 BFA annual meeting will be held from Friday to Sunday with the theme of \"Green Asia: Moving Towards Win-Win Through Changes \".\n",
            "\n",
            "- got: <UNK> <UNK> can also open up a <UNK> <UNK> and let the car be used to create <UNK> and <UNK> for system <UNK>\n",
            "\n",
            "\n",
            "- source: 对于 很 有 天赋 的 团队 来说 ， 最终 能 一路 打入 决赛 也 是 千载难逢 的 机会 。\n",
            "\n",
            "- expected: There is also the chance of a lifetime for the talented teams who win through to the final.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 雷诺 的 合并 和 关闭 道路 的 能力 ， 再 加上 一个 考虑 周全 的 内部 相当 多功能 帮助 它 获得 通过 。\n",
            "\n",
            "- expected: Renault's combination of off and on- road ability, plus a well thought out interior giving considerable versatility helped it win through.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 1974 年 至 1988 年 期间 ， 缅甸政府 通过 缅甸 社会主义 纲领 院在奈温 的 领导 下 有效 的 运转 。\n",
            "\n",
            "- expected: Between 1974 and 1988, Burma was effectively ruled by Ne Win through the Burma Socialist Programme Party (BSPP)\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 要 做到 这 一点 ， 银行 的 配套改革 必须 同步进行 ， 以 消除 实施 新 协议 的 各种 障碍 。\n",
            "\n",
            "- expected: At the same time, the banking compound reform must be in practice so as to win through all the difficulties during practice of new accord.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 在 知识经济 时代 ， 学习 速度 小于 变化 速度 就 等于 死亡 ， 学习型 组织 被誉为 未来 最 成功 企业 的 组织 模式 。\n",
            "\n",
            "- expected: In the era of knowledge economy, with the change of the circumstance speeding up, knowledge has become the key point to win through the competition.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 然而 ， 要 真正 做到 保护 鲨鱼 ， 环保 人士 们 还 需要 说服 亚洲 的 鱼翅 消费者 ， 改变 他们 对于 鱼翅 汤 的 偏好 。\n",
            "\n",
            "- expected: To truly save sharks, however, conservationists have to win over consumers in Asia and change the image of shark - fin soup.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 为了 赢得 对 美国 的 怀疑 ， 野田 佳彦要 说服 他 的 同僚 们 已 取得 足够 的 国内 支持 和 信任 去 谈判 。\n",
            "\n",
            "- expected: To win over American sceptics, Mr Noda will need to convince his counterparts that he has enough domestic support to negotiate in good faith.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> together to create one of the world's most <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 正确 做法 ： 通过 作为 团队 成员 来 赢得 老板 和 同事 的 认可 。\n",
            "\n",
            "- expected: The right approach: Win over the boss and colleagues by being a team player.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and so on.\n",
            "\n",
            "\n",
            "- source: 但是 他们 需要 说服 茶党 ， 该党 声称 他们 一直 批评 答不溜 长期 肆意挥霍 直到 奥巴马 执政 。\n",
            "\n",
            "- expected: But they needed to win over Tea Partiers, who claim that they were criticizing Dubya ’s profligacy long before President Obama took office.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> from <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 去年 在 谢菲尔德 ， 亨得利 打败 了 丁俊晖 ， 而 丁 可能 还 在 回味 2005 年 中国 公开赛 上 与 这位 苏格兰人 交战 的 胜利 。\n",
            "\n",
            "- expected: Hendry beat Ding in Sheffield last year, but Ding must still savour memories of his win over the Scotsman in the 2005 China Open.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 注意 到 你 在 辛辛那提 赢 了 佩特洛 娃 的 时候 膝盖 受伤 了 。\n",
            "\n",
            "- expected: We watched you play after you suffered the knee injury in your win over Petrova in Cincinnati.\n",
            "\n",
            "- got: <UNK> I go up to the <UNK> party but I can't get up the nerve to make a face.\n",
            "\n",
            "\n",
            "- source: 他 坚持 认为 ， 这是 一次性 的 ， 暂时 赢得 广场 行星 的 支持 ， 而 不是 相当 大规模 地 改写 规则 。\n",
            "\n",
            "- expected: This is a onetime, temporary win over the square planet, not some wholesale rewriting of the rules, he insists.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> in their <UNK> and <UNK> and sometimes <UNK>\n",
            "\n",
            "\n",
            "- source: 假如 你 想 成为 一个 优秀 的 时尚 设计师 ， 那么 朝着 目标 努力 就 应该 胜过 其他 一切 杂务 。\n",
            "\n",
            "- expected: If you want to excel as a fashion designer, working towards that goal should generally win over other other distractions.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 由于 国内 市场 受到限制 ， 这位 烟草 巨人 继续 致力于 在 新兴 市场推广 业务 ， 从 亚洲 到 俄罗斯 ， 赢得 数以 百万 的 烟枪 。\n",
            "\n",
            "- expected: As marketing restrictions tighten at home, the cigarette giant continues to push hard in emerging markets from Asia to Russia and win over millions of smokers.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 但是 他 承认 ， 要 把 坚定 的 保守派 争取 过来 ， 还有 很长 的 一段路 要 走 。\n",
            "\n",
            "- expected: But he acknowledged he still has a long way to go to win over staunch conservatives.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 随着 中国 的 现代化 ， 该 地区 的 人们 希望 这 两个 巨人 之间 能够 通过 友好 的 竞争 来 赢得 邻居 。\n",
            "\n",
            "- expected: With China's modernization all in the region hope these two giants will remain in friendly competition to win over their neighbors.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> <UNK> and are the <UNK> of <UNK>\n",
            "\n",
            "\n",
            "- source: 不幸 的 是 ， 只要 经济 事务 还 占 主流 ， 实用主义 者 们 就 占上风 。\n",
            "\n",
            "- expected: Unfortunately, so long as economic matters dominate, functionalists may well win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> from <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 逻辑 可能 在 今天 并 不能 获胜 ， 但是 在 大多数 组织 中 ， 它 并 不会 完全 被 忽略 。\n",
            "\n",
            "- expected: Logic won't win the day, but in most organizations, it's not completely ignored.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> have all been <UNK> over the past 10 years.\n",
            "\n",
            "\n",
            "- source: 这种 开放 的 方式 ， 再 加上 办公室 短缺 ， 使得 大家 能 在 一个 空间 里 发表 任何 可以 成功 的 点子 。\n",
            "\n",
            "- expected: This access to information, coupled with the lack of offices, created a flat structure where any idea could win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 但 这个 信经 并 没有 成为 主流 因为 在 整个 四 世纪 , 主教 之间 仍然 有 分歧 ， 有些 人 并 不 接受 尼西亚信 经 。\n",
            "\n",
            "- expected: It didn't win the day because throughout the fourth century you still had fights among different bishops, some people not accepting the Nicene Creed.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> or <UNK>\n",
            "\n",
            "\n",
            "- source: 但是 可爱 并 没有 获胜 ， 而 不幸 的 故事 也 没有 获胜 。\n",
            "\n",
            "- expected: But winsome didn't win the day, and neither did hard luck stories.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> from <UNK> and <UNK>\n",
            "\n",
            "\n",
            "- source: 但 作为 一个 投资人 ， 我 倾向 以 短期 思路 把 钱 投去 赚 快 钱 。\n",
            "\n",
            "- expected: As an investor, though, I'd put my money on short-term thinking to win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 它 仅仅 是 做 了 一个 假设 ： 更好 的 数据 加上 更好 的 分析 工具 将会 赢得 世界 。\n",
            "\n",
            "- expected: It didn't pretend to know anything about the culture and conventions of advertising —it just assumed that better data, with better analytical tools, would win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 使 他 取得成功 的 是 他 的 决心 ， 他 不甘 屈服 的 精神 ， 也 可能 还有 他 的 正直 。\n",
            "\n",
            "- expected: His determination, his refusal to yield, and possibly his integrity had helped him win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 把 他们 从 这里 赶出去 ！ - 只要 我们 在 今天 获胜 ， 七月 四日 就 不会 被称作 美国 的 假日 。\n",
            "\n",
            "- expected: Get'em out of there! - Should we win the day, the Fourth of July will no longer be known as an American holiday.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> or <UNK>\n",
            "\n",
            "\n",
            "- source: 当 你 越是 身陷 绝境 ， 你 的 意志 就 越发 坚强 。 你 只会 更加 努力 地 战斗 ， 并 确信 自己 终将 获得胜利 。\n",
            "\n",
            "- expected: Your will only grows stronger when your back is against the wall, and you fight all the harder, making sure you win the day.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 它 的 微妙 之 处 ， 将 胜利 的 日子 ， 这是 新车 。\n",
            "\n",
            "- expected: It's the subtlety that will win the day for this car.\n",
            "\n",
            "- got: <UNK> I think I'll go and knock around in <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 在 竞争 激烈 的 社会 里 获得成功 说 起来 容易 做 起来 难 ， 但是 她 已经 做到 了 。\n",
            "\n",
            "- expected: To win the day in the highly competitive society is easier said than done. But she has made it.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 你 影响 了 最初 我 曾经 有过 的 一些 疑问 。\n",
            "\n",
            "- expected: You win on most original question I think I've ever had.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 如果 他 说 的 是 真的 ， 将 意味着 这位 总统 的 选票 数远 低于 第一轮 选举 获胜 所 要求 的 50% 的 选票 。\n",
            "\n",
            "- expected: If true, that would mean the president received well under the 50% of all votes required for him to win on the first round.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> from <UNK>\n",
            "\n",
            "\n",
            "- source: 我 相信 利物浦 将 在 周六 获胜 ， 因为 他们 已经 有 在 压力 下 作战 的 技巧 ， 也 有 获胜 的 愿望 和 经验 。\n",
            "\n",
            "- expected: I think Liverpool will win on Saturday, because they have the knack of playing under pressure, the desire and the experience.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all so <UNK> as to one another and the <UNK> is <UNK>\n",
            "\n",
            "\n",
            "- source: 我 想 我 的 球员 都 认为 我们 最终 能 取得 一场 客场 胜利 。\n",
            "\n",
            "- expected: I think our guys just assumed we would eventually win on the road.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> and <UNK> and <UNK> and <UNK> and <UNK> are all clear of this <UNK>\n",
            "\n",
            "\n",
            "- source: 移动 时代 创造 了 HTML5 等 的 开放 新 标准 ， 它们 将 在 移动 设备 （ 还有 个人 计算机 ） 上 获胜 。\n",
            "\n",
            "- expected: New open standards created in the mobile era, such as HTML5, will win on mobile devices (and PCs too).\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 奥巴马 以及 共和党 的 候选人 麦凯恩 ， 都 正在 为 赢得 佛罗里达州 做 最后 的 努力 。 佛州 将 是 赢得 下星期 美国 总统大选 的 关键 州 。\n",
            "\n",
            "- expected: Both Obama and the Republican contender, John McCain, are making late drives to win Florida, which will be a key state to win on Election Day next week.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 宾夕法尼亚州 初选 过后 的 星期三 早晨 ， 克林顿 参议员 在 哥伦比亚 广播公司 的 早间 节目 中 总结 了 自己 赢得 超过 对手 10% 选票 的 感受 。\n",
            "\n",
            "- expected: On the morning after the Pennsylvania primary, Senator Hillary Clinton summed up her feelings about her nearly 10-point win on the CBS Early Show.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 而 给予 网队 球迷 最好 的 礼物 是 网队 能 在 圣诞夜 赢得 比赛 。\n",
            "\n",
            "- expected: The only thing better than Yi's return for New Jersey fans would be a Nets' win on Christmas Eve.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 每 一个 投资者 ， 无论是 初学者 还是 经验丰富 者 ， 都 可以 在 股票市场 中 通过 这些 准则 或 得 胜利 。\n",
            "\n",
            "- expected: Every investor, whether a novice or very experienced, will be able to win on the stock market with these rules.\n",
            "\n",
            "- got: <UNK> <UNK> I can go through fire and water to protect me.\n",
            "\n",
            "\n",
            "- source: 现在 如果 我们 要 取得 最终 的 比赛 胜利 ， 我们 就 必须 取得 一场 客场 比赛 胜利 ， 事情 就是 这样 的 。\n",
            "\n",
            "- expected: And if we want what we want, we have to win on the road and that's just the way it is.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> and <UNK> and <UNK> are all <UNK>\n",
            "\n",
            "\n",
            "- source: 如果 我们 在 周六 获胜 ， 下一场 对 曼联 的 时候 我们 将 有 一个 绝好 的 机会 扭转 目前 的 局面 。\n",
            "\n",
            "- expected: If we win on Saturday and the next game against United that would be very good for us because we can change our situation.\n",
            "\n",
            "- got: <UNK> <UNK> I can go up to the <UNK> for <UNK>\n",
            "\n",
            "\n",
            "- source: 你 最好 放 聪明 一点 ， 告诉 我 钱 放在 哪里 。\n",
            "\n",
            "- expected: You had better wise up, tell me where the money is.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 人类 在 生命 早期 并 不明智 ， 这 听 起来 有点 疯狂 。\n",
            "\n",
            "- expected: It can seem a bit crazy that we humans don't wise up a bit earlier in life.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 如果 有 一个 任何 新任 经理 们 应该 迅速 识破 的 关于 工作 场合 的 大 谎言 的话 ， 它 必然 是 “ 这里 没有 办公室 政治 。 ”\n",
            "\n",
            "- expected: If there’s one big workplace lie that any new managershould wise up to fast, it ’s “There are no office politicshere .”\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all so <UNK> and <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 欧洲人 应该 去 了解 中国 ， 尊敬 中国 ， 而 不是 因为 无知 而 对 它 有 畏惧感 。\n",
            "\n",
            "- expected: Europeans need to wise up to China and treat it with respect rather than awe born of ignorance.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> and <UNK> have all been <UNK> into <UNK>\n",
            "\n",
            "\n",
            "- source: 在 电子管 预热 前 一会 ， 埃维 还 赶得及 说 ， “ 明白 点事 吧 ， 佛 雷德 。 我们 以后 该 怎么办 ？ ”\n",
            "\n",
            "- expected: In the moment before the tubes warmed up, Evey had tiem to say, “Wise up, Freddy. What shall we do?\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 我们 公民 需要 明智 起来 采取任何 必要 的 措施 “ 改变 或 废除 ” 我们 的 联邦政府 。\n",
            "\n",
            "- expected: We citizens need to wise up and \"alter or abolish\" our federal government by any means necessary.\n",
            "\n",
            "- got: <UNK> <UNK> and <UNK> are all <UNK> <UNK> and are the ones that turn you out to be sent in <UNK>\n",
            "\n",
            "\n",
            "- source: 这 不是 独角戏 啊 ， 孩子 。 你 要 放 聪明 点 ， 给 自己 找 一个 一流 的 总机 。\n",
            "\n",
            "- expected: This ain't a one -man deal, kid. You need to wise up and get yourself.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 这 不是 独角戏 啊 ， 孩子 。 你 要 放 聪明 点 ， 给 自己 找 一个 一流 的 总机 师 和 伙伴 。\n",
            "\n",
            "- expected: This ain't one- man deal, kid. You need to wise up and get yourself a good crew chief and a good team.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 你 要 放 聪明 点 ， 给 自己 找 一个 一流 的 总机 师 和 伙伴 。\n",
            "\n",
            "- expected: You need to wise up and get yourself a good crew chief and a good team.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "- source: 这 不是 独角戏 啊 ， 孩子 。 你 要 放 聪明 点 ， 给 自己 找 一个 一流 的 总机 师 和 伙伴 。\n",
            "\n",
            "- expected: This ain't a one- man deal, kid. You need to wise up and get yourself a good crew chief and a good team.\n",
            "\n",
            "- got: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "\n",
            "\n",
            "[[['The', 'old', 'prefessor', 'came', 'back', 'to', 'Chine', 'in', '1949', 'when', 'the', 'mainland', 'had', 'just', 'been', 'liberated.']], [['The', 'whole', 'society', 'should', 'be', 'concerned', 'about', 'the', 'health', 'and', 'sound', 'growth', 'of', 'children.']], [['They', 'gave', 'their', 'lives', 'for', 'their', 'country', 'and', 'were', 'honoured', 'as', 'heroes.']], [['They', 'must', 'have', 'worn', 'gloves', 'in', 'order', 'not', 'to', 'leave', 'any', 'fingerprints.']], [['Thousands', 'of', 'people', 'were', 'starved', 'to', 'death', 'during', 'the', 'famine', 'year.']], [['To', 'get', 'into', 'university', 'you', 'have', 'to', 'pass', 'a', 'number', 'of', 'examinations.']], [['We', 'are', 'walking', 'at', 'the', 'speed', 'of', 'one', 'mile', 'at', 'hour.']], [['We', 'have', 'invite', 'him', 'here', 'to', 'speak,', 'so', \"we'd\", 'better', 'go', 'to', 'his', 'lecture.']], [['We', 'would', 'rather', 'live', 'in', 'the', 'suburbs', 'than', 'in', 'the', 'city.']], [['We', 'judge', 'a', 'person', 'not', 'by', 'what', 'he', 'says', 'but', 'by', 'what', 'he', 'does.']], [['A', 'man', 'should', 'be', 'judged', 'by', 'his', 'deeds,', 'not', 'his', 'words.']], [['If', 'you', 'can', 'use', 'a', 'word', 'correctly', 'and', 'effectively,', 'you', 'comprehend', 'it.']], [['He', 'floored', 'his', 'opponent', 'with', 'a', 'fine', 'punch', 'in', 'the', 'first', 'round.']], [['They', 'presented', 'a', 'sum', 'of', 'money', 'to', 'the', 'college', 'in', 'memory', 'of', 'their', 'son.']], [['I', 'will', 'not', 'breathe', 'a', 'word', 'of', 'your', 'secret', 'to', 'another', 'man.']], [[\"I'm\", 'sure', 'people', 'will', 'think,', 'Aah,', \"it's\", 'because', \"I've\", 'slept', 'with', 'Tim.']], [['The', 'standard', 'English', 'accent,', 'the', 'a', '(for', 'example', 'in', 'father)', 'is', 'pronounced', 'aah,', 'not', 'like', 'a', 'like', 'apple.']], [['They', 'say', '\"ooh\"', 'and', '\"aah\"', 'when', 'the', 'electric', 'company', 'turns', 'on', 'the', 'thousands', 'of', 'little', 'lights', 'in', 'the', 'ball.']], [['Objective', 'Diagnosis', 'and', 'differential', 'diagnosis', 'of', 'atypical', 'adenomatous', 'hyperplasia', 'of', 'the', 'prostate', '(AAH)', 'were', 'studied.']], [['It', 'has', 'been', 'proposed', 'that', 'stepwise', 'progression', 'occurs', 'from', 'atypical', 'adenomatous', 'hyperplasia', '(AAH)', 'through', 'bronchioloalveolar', 'carcinoma', '(BAC)', 'to', 'invasive', 'lung', 'adenocarcinoma.']], [['Atypical', 'Adenomatous', 'Hyperplasia', '(AAH)', 'of', 'the', 'human', 'lung', 'has', 'been', 'recently', 'implicated', 'as', 'a', 'possible', 'precursor', 'lesion', 'of', 'bronchioloalveolar', 'carcinoma', '(BAC).']], [['Moreover,', 'an', 'improvement', 'on', 'the', 'traditional', 'histogram', 'called', 'the', 'average', 'area', 'histogram', '(AAH)', 'is', 'proposed', 'to', 'represent', 'color', 'features.']], [['They', 'ooh', 'and', 'aah', 'when', 'the', 'thousands', 'of', 'little', 'lights', 'in', 'the', 'ball', 'come', 'on.']], [['AAH', 'has', 'been', 'shown', 'to', 'have', 'immunohistochemical,', 'morphometric,', 'flow', 'cytometric', 'and', 'genetic', 'abnormalities', 'overlapping', 'with', 'adenocarcinoma.']], [['Aah,', \"that's\", 'not', 'so', 'bad,', 'Walden', 'thought.', 'That', 'we', 'might', 'be', 'able', 'to', 'manage.']], [['They', 'ooh', 'and', 'aah', 'when', 'the', 'thousands', 'of', 'little', 'lights', 'in', 'the', 'ball', 'come', 'on.']], [['It', 'is', 'equipped', 'with', 'shotguns,', 'tear', 'gas,', 'a', 'night-', 'vision', 'camera', 'and', 'bags', 'of', \"Obama's\", 'blood', '(group', 'AB).']], [['My', 'grammar', 'finds', 'an', 'ABC', 'when', 'it', 'is', 'available,', 'but', 'settles', 'for', 'an', 'A,', 'or', 'AB,', 'when', 'that', 'is', 'all', 'there', 'is', 'to', 'find.']], [['Notice', 'that', 'alternatives', 'within', 'the', '(ABC', '|', 'AB', '|', 'A)', 'alternation', 'are', 'all', 'tried', 'before', 'the', 'parser', 'looks', 'for', 'the', 'trailing', 'words.']], [['A', 'search', 'for', 'a', '3', '-', 'letter', 'word,', 'like', 'ABC,', 'can', 'be', 'done', 'by', 'searching', 'for', 'AB', 'and', 'BC.']], [['So,', 'now', 'we', 'can', 'answer', 'the', 'initial', 'question', 'because', 'vector', 'OP,', 'well,', 'we', 'just', 'add', 'up', 'OA,', 'AB,', 'and', 'BP.']], [['Next,', 'get', 'a', 'listing', 'of', 'the', '/tmp', 'contents', 'in', 'the', 'root', 'terminal;', 'notice', 'that', 'there', 'are', 'likely', 'other', 'files', 'but', 'no', '/tmp/ab.']], [['The', 'ab', 'utility', 'is', 'simple', 'to', 'use:', 'Provide', 'a', 'repeat', 'count', 'and', 'a', 'URL.']], [['Use', 'ab', 'to', 'automate', 'a', 'large', 'number', 'of', 'requests', 'for', 'PHP', 'pages.']], [['The', 'ab', 'utility', 'requests', 'that', 'URL', 'many', 'times', 'over', 'and', 'returns', 'statistics.']], [['Descending', 'into', 'the', 'new', 'exhibition,', 'hard-core', 'science', 'fiction', 'fans', 'may', 'be', 'taken', 'aback', 'at', 'some', 'of', 'the', 'work', 'around', 'them.']], [['But', 'he', 'might', 'be', 'taken', 'aback', 'by', 'the', 'popular', 'sympathy', 'for', 'his', 'plight.']], [['While', 'I', 'was', 'initially', 'taken', 'aback', 'by', 'negativity', 'of', 'the', 'people,', 'I', 'eventually', 'learned', 'to', 'manage', 'it', 'and', 'channel', 'it', 'into', 'conscious', 'action.']], [['I', 'think', \"we're\", 'taken', 'aback', 'here.', \"We're\", 'taken', 'aback', 'by', 'the', 'rude', 'interruption', 'in', 'the', 'otherwise', 'seamless', 'flow', 'of', 'the', \"poet's\", 'lament.']], [['Many', 'Samaritans', 'who', 'met', 'him', 'on', 'his', 'travels,', 'as', 'well', 'as', 'journalists', 'and', 'others,', 'were', 'rather', 'taken', 'aback', 'by', 'Mr', 'Varah', 'in', 'person.']], [['But', 'even', 'he', 'might', 'have', 'been', 'taken', 'aback', 'by', 'the', 'recipe', 'concocted', 'by', 'Peter', 'Ferlow.']], [['You', 'should', 'not', 'be', 'taken', 'aback', 'with', 'some', 'issues', 'that', 'might', 'put', 'you', 'off', 'at', 'first', 'reading.']], [['I', 'enjoy', 'speed,', '”', 'Jonathan', 'said,', 'taken', 'aback', 'but', 'proud', 'that', 'the', 'Elder', 'had', 'noticed.']], [['I,', 'and', 'several', 'of', 'the', 'other', 'young', 'professionals', 'in', 'our', 'group,', 'were', 'taken', 'aback', 'by', 'his', 'casual', 'remark.']], [['Many', 'of', 'you', 'feel', 'that', 'returning', 'responsibility', 'to', 'people', 'in', 'this', 'way', 'means', 'to', 'abandon', 'them', 'or', 'to', 'tell', 'them', 'to', 'solve', 'the', 'issue', 'all', 'by', 'themselves.']], [['Letting', 'Go:', 'I', 'think', 'educated', 'people', 'find', 'it', 'harder', 'to', 'just', 'accept,', 'to', 'abandon', 'themselves', 'and', 'their', 'ideas,', 'to', 'let', 'go', 'of', 'what', 'control', 'they', 'have.']], [['Many', 'of', 'you', 'feel', 'that', 'returning', 'responsibility', 'to', 'people', 'in', 'this', 'way', 'means', 'to', 'abandon', 'them', 'or', 'to', 'tell', 'them', 'to', 'solve', 'the', 'issue', 'all', 'by', 'themselves.']], [['Those', 'critical', 'of', 'encryption', 'might', 'suggest', 'that', 'law-', 'abiding', 'citizens', 'have', 'nothing', 'to', 'hide,', 'but', 'that', 'simply', \"isn't\", 'true.']], [['To', 'the', 'extent', 'that', 'OK', 'Cupid', 'has', 'any', 'abiding', 'faith,', 'it', 'is', 'in', 'mathematics.']], [['So', 'abiding', 'in', 'his', 'love', 'means', 'continuing', 'to', 'believe,', 'moment', 'by', 'moment,', 'that', 'we', 'are', 'loved.']], [['After', 'you', 'create', 'a', 'project,', 'you', 'wind', 'up', 'with', 'a', 'directory', 'structure', 'similar', 'to', 'the', 'one', 'illustrated', 'in', 'Figure', '2.']], [['How', 'do', 'these', 'actually', 'wind', 'up', 'in', 'the', 'spinner,', 'and', 'how', 'can', 'the', 'application', 'toggle', 'between', 'English', 'and', 'metric', 'units?']], [['The', 'question', 'now', 'is', 'which', 'of', 'the', 'deep', 'pocketed', 'groups', 'listed', 'above', '—', 'if', 'any', '—', 'will', 'wind', 'up', 'partnering', 'with', 'Huffington.']], [['Its', 'no', 'wonder,', 'then,', 'that', 'so', 'many', 'young,', 'single', 'partiers', 'wind', 'up', 'sleeping', 'with', 'new', 'people', 'after', 'a', 'long', 'night', 'of', 'guzzling', 'beer.']], [['He', 'was', 'expected', 'to', 'win', 'in', 'a', 'walk,', 'but', 'he', 'was', 'badly', 'beaten.']], [['Walk', 'can', 'be', 'a', 'win', '-win', 'excise', 'in', 'your', 'daily', 'life,', 'which', 'will', 'assist', 'ur', 'digestion', 'and', 'save', 'ur', 'Exercise', 'time.']], [[\"There's\", 'still', 'one', 'game', 'to', 'go', 'and', 'hopefully', 'they', '(Spurs)', 'will', 'draw', 'next', 'week', 'and', 'we', 'win', 'and', 'get', 'into', 'Europe', 'next', 'season.']], [['Ultimately,', 'it', 'becomes', 'clear', 'that', \"there's\", 'only', 'one', 'sure', 'way', 'to', 'win', 'the', 'election:', 'kill', 'the', 'other', 'guy.', 'And', 'the', 'battle', 'begins.']], [['No', 'one', 'thought', 'it', 'could', 'happen', '--', \"there's\", 'no', 'way', 'this', 'debt', '-laden,', 'cash', 'strapped', 'Italian', 'company', 'will', 'win', 'in', 'an', 'international', 'court', 'of', 'law.']], [['The', 'way', \"he's\", 'going,', 'he', 'is', 'one', 'of', 'the', 'favourites', 'and', 'if', 'he', \"doesn't\", 'win', 'it', 'this', 'year,', \"he'll\", 'certainly', 'win', 'it', 'at', 'some', 'point.']], [['Only', 'once', 'Burns', 'became', 'so', 'wrapped', 'up', 'in', 'Washington', 'politics', 'that', 'he', 'could', 'no', 'longer', 'care', 'did', 'Rothbard', 'finally', 'win', 'out.']], [['Which', 'neoconservative', 'impulse', 'will', 'win', 'out', '--', 'the', 'embrace', 'of', 'democratic', 'longing,', 'or', 'the', 'fear', 'of', 'Islamic', 'movements', 'taking', 'power?']], [['We', 'have', 'to', 'create', 'in', 'children', 'a', 'deep', 'seated', 'need', 'for', 'books,', 'otherwise', 'television', 'will', 'always', 'win', 'out.']], [['“At', 'times', 'when', 'there', 'is', 'more', 'inflation', 'pressure,', 'the', 'politburo', 'is', 'primarily', 'concerned', 'about', 'inflation', ',”', 'Hui', 'says.“', \"That's\", 'when', 'those', 'arguing', 'for', 'more', 'flexibility', 'tend', 'to', 'win', 'out.”']], [['But', 'if', 'muddle', 'and', 'jealousy', 'win', 'out,', 'nuclear', 'proliferation,', 'not', 'restraint,', 'will', 'be', 'the', 'norm—to', 'enduring', 'regret', 'all', 'round.']], [['It', 'seems', 'rather', 'intuitive', 'that', 'crucibles', 'of', 'burning', 'need', 'win', 'out', 'over', 'eggheads,', 'and', 'I', 'suppose', 'that', 'intuition', 'stands', 'up', 'well', 'to', 'closer', 'inspection.']], [['The', 'slow', 'pace', 'of', 'recovery', 'raises', 'the', 'question:', 'Which', 'Japan', 'will', 'win', 'out', 'in', 'the', 'aftermath', 'of', 'the', 'tragedy?']], [['We', 'protect', 'free', 'speech', 'with', 'the', 'force', 'of', 'law,', 'and', 'we', 'appeal', 'to', 'the', 'force', 'of', 'reason', 'to', 'win', 'out', 'over', 'hate.']], [['And', 'I', 'couldn', '’t', 'get', 'Daddy', 'and', 'his', 'confident', 'deathbed', 'prediction', 'that', 'I', 'would', 'win', 'out', 'of', 'my', 'mind.']], [['But', 'it', 'will', 'take', 'a', 'miracle', 'for', 'market', 'forces', 'to', 'win', 'out', 'over', 'political', 'fear.']], [['Informal', 'polls', 'and', 'interviews', 'generally', 'result', 'in', 'the', 'concession', 'that', 'when', 'it', 'comes', 'to', 'body', 'odors,', 'vegetarians', 'usually', 'win', 'out.']], [['No', 'matter', 'how', 'long', 'it', 'may', 'take', 'us', 'to', 'overcome', 'this', 'premeditated', 'invasion,', 'the', 'American', 'people', 'in', 'their', 'righteous', 'might', 'will', 'win', 'through', 'to', 'absolute', 'victory.']], [['It’s', 'not', 'an', 'easy', 'task', 'for', 'the', 'contestants', 'to', 'win', 'through', 'the', 'fierce', 'competition.']], [[\"It's\", 'going', 'to', 'be', 'hard', 'work', 'but', \"we'll\", 'win', 'through', 'in', 'the', 'end.']], [['He', 'has', 'weathered', 'many', 'political', 'storms,', 'using', 'his', 'political', 'wit', 'to', 'win', 'through', 'whee', 'other', 'politician', 'would', 'have', 'faltered.']], [[\"Don't\", 'try', 'to', 'slur', 'over', 'your', 'weaknesses', 'in', 'English', 'learning;', 'only', 'by', 'facing', 'up', 'to', 'them,', 'can', 'you', 'win', 'through', 'in', 'the', 'end.']], [[\"Nothing's\", 'impossible', 'and', 'it', 'all', 'depends', 'on', 'man;', 'you', 'would', 'win', 'through', 'if', 'you', 'persist.']], [['\"In', 'addition', 'to', 'music', 'therapy,', 'music', 'therapists', 'will', 'often', 'pop', 'into', 'the', 'selection', 'of', 'some', 'of', 'the', 'sound', 'library', 'treatment,', 'such', 'as\"', 'eager', 'to', 'win', 'through', 'struggle.']], [['The', '2008', 'BFA', 'annual', 'meeting', 'will', 'be', 'held', 'from', 'Friday', 'to', 'Sunday', 'with', 'the', 'theme', 'of', '\"Green', 'Asia:', 'Moving', 'Towards', 'Win-Win', 'Through', 'Changes', '\".']], [['There', 'is', 'also', 'the', 'chance', 'of', 'a', 'lifetime', 'for', 'the', 'talented', 'teams', 'who', 'win', 'through', 'to', 'the', 'final.']], [[\"Renault's\", 'combination', 'of', 'off', 'and', 'on-', 'road', 'ability,', 'plus', 'a', 'well', 'thought', 'out', 'interior', 'giving', 'considerable', 'versatility', 'helped', 'it', 'win', 'through.']], [['Between', '1974', 'and', '1988,', 'Burma', 'was', 'effectively', 'ruled', 'by', 'Ne', 'Win', 'through', 'the', 'Burma', 'Socialist', 'Programme', 'Party', '(BSPP)']], [['At', 'the', 'same', 'time,', 'the', 'banking', 'compound', 'reform', 'must', 'be', 'in', 'practice', 'so', 'as', 'to', 'win', 'through', 'all', 'the', 'difficulties', 'during', 'practice', 'of', 'new', 'accord.']], [['In', 'the', 'era', 'of', 'knowledge', 'economy,', 'with', 'the', 'change', 'of', 'the', 'circumstance', 'speeding', 'up,', 'knowledge', 'has', 'become', 'the', 'key', 'point', 'to', 'win', 'through', 'the', 'competition.']], [['To', 'truly', 'save', 'sharks,', 'however,', 'conservationists', 'have', 'to', 'win', 'over', 'consumers', 'in', 'Asia', 'and', 'change', 'the', 'image', 'of', 'shark', '-', 'fin', 'soup.']], [['To', 'win', 'over', 'American', 'sceptics,', 'Mr', 'Noda', 'will', 'need', 'to', 'convince', 'his', 'counterparts', 'that', 'he', 'has', 'enough', 'domestic', 'support', 'to', 'negotiate', 'in', 'good', 'faith.']], [['The', 'right', 'approach:', 'Win', 'over', 'the', 'boss', 'and', 'colleagues', 'by', 'being', 'a', 'team', 'player.']], [['But', 'they', 'needed', 'to', 'win', 'over', 'Tea', 'Partiers,', 'who', 'claim', 'that', 'they', 'were', 'criticizing', 'Dubya', '’s', 'profligacy', 'long', 'before', 'President', 'Obama', 'took', 'office.']], [['Hendry', 'beat', 'Ding', 'in', 'Sheffield', 'last', 'year,', 'but', 'Ding', 'must', 'still', 'savour', 'memories', 'of', 'his', 'win', 'over', 'the', 'Scotsman', 'in', 'the', '2005', 'China', 'Open.']], [['We', 'watched', 'you', 'play', 'after', 'you', 'suffered', 'the', 'knee', 'injury', 'in', 'your', 'win', 'over', 'Petrova', 'in', 'Cincinnati.']], [['This', 'is', 'a', 'onetime,', 'temporary', 'win', 'over', 'the', 'square', 'planet,', 'not', 'some', 'wholesale', 'rewriting', 'of', 'the', 'rules,', 'he', 'insists.']], [['If', 'you', 'want', 'to', 'excel', 'as', 'a', 'fashion', 'designer,', 'working', 'towards', 'that', 'goal', 'should', 'generally', 'win', 'over', 'other', 'other', 'distractions.']], [['As', 'marketing', 'restrictions', 'tighten', 'at', 'home,', 'the', 'cigarette', 'giant', 'continues', 'to', 'push', 'hard', 'in', 'emerging', 'markets', 'from', 'Asia', 'to', 'Russia', 'and', 'win', 'over', 'millions', 'of', 'smokers.']], [['But', 'he', 'acknowledged', 'he', 'still', 'has', 'a', 'long', 'way', 'to', 'go', 'to', 'win', 'over', 'staunch', 'conservatives.']], [['With', \"China's\", 'modernization', 'all', 'in', 'the', 'region', 'hope', 'these', 'two', 'giants', 'will', 'remain', 'in', 'friendly', 'competition', 'to', 'win', 'over', 'their', 'neighbors.']], [['Unfortunately,', 'so', 'long', 'as', 'economic', 'matters', 'dominate,', 'functionalists', 'may', 'well', 'win', 'the', 'day.']], [['Logic', \"won't\", 'win', 'the', 'day,', 'but', 'in', 'most', 'organizations,', \"it's\", 'not', 'completely', 'ignored.']], [['This', 'access', 'to', 'information,', 'coupled', 'with', 'the', 'lack', 'of', 'offices,', 'created', 'a', 'flat', 'structure', 'where', 'any', 'idea', 'could', 'win', 'the', 'day.']], [['It', \"didn't\", 'win', 'the', 'day', 'because', 'throughout', 'the', 'fourth', 'century', 'you', 'still', 'had', 'fights', 'among', 'different', 'bishops,', 'some', 'people', 'not', 'accepting', 'the', 'Nicene', 'Creed.']], [['But', 'winsome', \"didn't\", 'win', 'the', 'day,', 'and', 'neither', 'did', 'hard', 'luck', 'stories.']], [['As', 'an', 'investor,', 'though,', \"I'd\", 'put', 'my', 'money', 'on', 'short-term', 'thinking', 'to', 'win', 'the', 'day.']], [['It', \"didn't\", 'pretend', 'to', 'know', 'anything', 'about', 'the', 'culture', 'and', 'conventions', 'of', 'advertising', '—it', 'just', 'assumed', 'that', 'better', 'data,', 'with', 'better', 'analytical', 'tools,', 'would', 'win', 'the', 'day.']], [['His', 'determination,', 'his', 'refusal', 'to', 'yield,', 'and', 'possibly', 'his', 'integrity', 'had', 'helped', 'him', 'win', 'the', 'day.']], [[\"Get'em\", 'out', 'of', 'there!', '-', 'Should', 'we', 'win', 'the', 'day,', 'the', 'Fourth', 'of', 'July', 'will', 'no', 'longer', 'be', 'known', 'as', 'an', 'American', 'holiday.']], [['Your', 'will', 'only', 'grows', 'stronger', 'when', 'your', 'back', 'is', 'against', 'the', 'wall,', 'and', 'you', 'fight', 'all', 'the', 'harder,', 'making', 'sure', 'you', 'win', 'the', 'day.']], [[\"It's\", 'the', 'subtlety', 'that', 'will', 'win', 'the', 'day', 'for', 'this', 'car.']], [['To', 'win', 'the', 'day', 'in', 'the', 'highly', 'competitive', 'society', 'is', 'easier', 'said', 'than', 'done.', 'But', 'she', 'has', 'made', 'it.']], [['You', 'win', 'on', 'most', 'original', 'question', 'I', 'think', \"I've\", 'ever', 'had.']], [['If', 'true,', 'that', 'would', 'mean', 'the', 'president', 'received', 'well', 'under', 'the', '50%', 'of', 'all', 'votes', 'required', 'for', 'him', 'to', 'win', 'on', 'the', 'first', 'round.']], [['I', 'think', 'Liverpool', 'will', 'win', 'on', 'Saturday,', 'because', 'they', 'have', 'the', 'knack', 'of', 'playing', 'under', 'pressure,', 'the', 'desire', 'and', 'the', 'experience.']], [['I', 'think', 'our', 'guys', 'just', 'assumed', 'we', 'would', 'eventually', 'win', 'on', 'the', 'road.']], [['New', 'open', 'standards', 'created', 'in', 'the', 'mobile', 'era,', 'such', 'as', 'HTML5,', 'will', 'win', 'on', 'mobile', 'devices', '(and', 'PCs', 'too).']], [['Both', 'Obama', 'and', 'the', 'Republican', 'contender,', 'John', 'McCain,', 'are', 'making', 'late', 'drives', 'to', 'win', 'Florida,', 'which', 'will', 'be', 'a', 'key', 'state', 'to', 'win', 'on', 'Election', 'Day', 'next', 'week.']], [['On', 'the', 'morning', 'after', 'the', 'Pennsylvania', 'primary,', 'Senator', 'Hillary', 'Clinton', 'summed', 'up', 'her', 'feelings', 'about', 'her', 'nearly', '10-point', 'win', 'on', 'the', 'CBS', 'Early', 'Show.']], [['The', 'only', 'thing', 'better', 'than', \"Yi's\", 'return', 'for', 'New', 'Jersey', 'fans', 'would', 'be', 'a', \"Nets'\", 'win', 'on', 'Christmas', 'Eve.']], [['Every', 'investor,', 'whether', 'a', 'novice', 'or', 'very', 'experienced,', 'will', 'be', 'able', 'to', 'win', 'on', 'the', 'stock', 'market', 'with', 'these', 'rules.']], [['And', 'if', 'we', 'want', 'what', 'we', 'want,', 'we', 'have', 'to', 'win', 'on', 'the', 'road', 'and', \"that's\", 'just', 'the', 'way', 'it', 'is.']], [['If', 'we', 'win', 'on', 'Saturday', 'and', 'the', 'next', 'game', 'against', 'United', 'that', 'would', 'be', 'very', 'good', 'for', 'us', 'because', 'we', 'can', 'change', 'our', 'situation.']], [['You', 'had', 'better', 'wise', 'up,', 'tell', 'me', 'where', 'the', 'money', 'is.']], [['It', 'can', 'seem', 'a', 'bit', 'crazy', 'that', 'we', 'humans', \"don't\", 'wise', 'up', 'a', 'bit', 'earlier', 'in', 'life.']], [['If', 'there’s', 'one', 'big', 'workplace', 'lie', 'that', 'any', 'new', 'managershould', 'wise', 'up', 'to', 'fast,', 'it', '’s', '“There', 'are', 'no', 'office', 'politicshere', '.”']], [['Europeans', 'need', 'to', 'wise', 'up', 'to', 'China', 'and', 'treat', 'it', 'with', 'respect', 'rather', 'than', 'awe', 'born', 'of', 'ignorance.']], [['In', 'the', 'moment', 'before', 'the', 'tubes', 'warmed', 'up,', 'Evey', 'had', 'tiem', 'to', 'say,', '“Wise', 'up,', 'Freddy.', 'What', 'shall', 'we', 'do?']], [['We', 'citizens', 'need', 'to', 'wise', 'up', 'and', '\"alter', 'or', 'abolish\"', 'our', 'federal', 'government', 'by', 'any', 'means', 'necessary.']], [['This', \"ain't\", 'a', 'one', '-man', 'deal,', 'kid.', 'You', 'need', 'to', 'wise', 'up', 'and', 'get', 'yourself.']], [['This', \"ain't\", 'one-', 'man', 'deal,', 'kid.', 'You', 'need', 'to', 'wise', 'up', 'and', 'get', 'yourself', 'a', 'good', 'crew', 'chief', 'and', 'a', 'good', 'team.']], [['You', 'need', 'to', 'wise', 'up', 'and', 'get', 'yourself', 'a', 'good', 'crew', 'chief', 'and', 'a', 'good', 'team.']], [['This', \"ain't\", 'a', 'one-', 'man', 'deal,', 'kid.', 'You', 'need', 'to', 'wise', 'up', 'and', 'get', 'yourself', 'a', 'good', 'crew', 'chief', 'and', 'a', 'good', 'team.']]]\n",
            "Bleu Score = 1.4551287660623549\n",
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}