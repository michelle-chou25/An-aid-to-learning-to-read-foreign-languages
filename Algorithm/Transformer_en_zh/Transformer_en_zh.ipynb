{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_en_zh.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkVZ9tB4w5gC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c284585b-91cd-4ea7-cefb-9c004a5e3612"
      },
      "source": [
        "# hyperparams\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import codecs\n",
        "import regex\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# data\n",
        "source_train = 'drive/MyDrive/Corpus/corpus_1w/english1w.txt'\n",
        "target_train = 'drive/MyDrive/Corpus/corpus_1w/chinese_out.txt'\n",
        "source_test = 'drive/MyDrive/Corpus/corpus_1w/test_english.txt'\n",
        "target_test = 'drive/MyDrive/Corpus/corpus_1w/test_chineseout.txt'\n",
        "\n",
        "# training\n",
        "batch_size = 32 # alias = N\n",
        "lr = 0.0001 # learning rate.\n",
        "logdir = 'drive/Shareddrives/ruochen.katherina@gmail.com/logdir_en_zh' # log directory\n",
        "\n",
        "# model\n",
        "maxlen = 30 # Maximum number of words in a sentence. alias = T.\n",
        "min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
        "hidden_units = 512 # alias = C\n",
        "num_blocks = 6 # number of encoder/decoder blocks\n",
        "num_epochs = 20 # traversal time\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "sinusoid = False # select embedding method"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPZINyqwyde8"
      },
      "source": [
        "def load_cn_vocab():\n",
        "    vocab = [line.split()[0] for line in codecs.open('drive/MyDrive/Corpus/corpus_1w/cn.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=min_cnt]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def load_en_vocab():\n",
        "    vocab = [line.split()[0] for line in codecs.open('drive/MyDrive/Corpus/corpus_1w/en.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=min_cnt]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def create_data(source_sents, target_sents): \n",
        "    en2idx, idx2en = load_en_vocab()\n",
        "    cn2idx, idx2cn = load_cn_vocab()\n",
        "    \n",
        "    # Index\n",
        "    x_list, y_list, Sources, Targets = [], [], [], []\n",
        "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
        "        x = [en2idx.get(word, 1) for word in (source_sent + u\" </S>\").split()] # 1: OOV, </S>: End of Text\n",
        "        y = [cn2idx.get(word, 1) for word in (target_sent + u\" </S>\").split()] \n",
        "        if max(len(x), len(y)) <=maxlen:\n",
        "            x_list.append(np.array(x))\n",
        "            y_list.append(np.array(y))\n",
        "            Sources.append(source_sent)\n",
        "            Targets.append(target_sent)\n",
        "    \n",
        "    # Pad      \n",
        "    X = np.zeros([len(x_list), maxlen], np.int32)\n",
        "    Y = np.zeros([len(y_list), maxlen], np.int32)\n",
        "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
        "        X[i] = np.lib.pad(x, [0, maxlen-len(x)], 'constant', constant_values=(0, 0))\n",
        "        Y[i] = np.lib.pad(y, [0, maxlen-len(y)], 'constant', constant_values=(0, 0))\n",
        "    \n",
        "    return X, Y, Sources, Targets\n",
        "\n",
        "def load_train_data():\n",
        "    cn_sents = [regex.sub(\"<[^>]+>\", \"\", line) for line in codecs.open(source_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
        "    en_sents = [regex.sub(\"<[^>]+>\", \"\", line) for line in codecs.open(target_train, 'r', 'utf-8').read().split(\"\\n\") if line and line[0] != \"<\"]\n",
        "    \n",
        "    X, Y, Sources, Targets = create_data(cn_sents, en_sents)\n",
        "    return X, Y\n",
        "    \n",
        "def load_test_data():\n",
        "    def _refine(line):\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        return line.strip()\n",
        "    \n",
        "    cn_sents = [_refine(line) for line in codecs.open(source_test, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "    en_sents = [_refine(line) for line in codecs.open(target_test, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "        \n",
        "    X, Y, Sources, Targets = create_data(cn_sents, en_sents)\n",
        "    return X, Sources, Targets # (1064, 150) \n",
        "\n",
        "def load_test_data1():\n",
        "    def _refine(line):\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
        "        return line.strip()\n",
        "    \n",
        "    cn_sents = [_refine(line) for line in codecs.open(source_test1, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "    en_sents = [_refine(line) for line in codecs.open(target_test1, 'r', 'utf-8').read().split(\"\\n\") if line ]\n",
        "        \n",
        "    X, Y, Sources, Targets = create_data(cn_sents, en_sents)\n",
        "    return X, Sources, Targets # (1064, 150) \n",
        "\n",
        "def get_batch_data():\n",
        "    X, Y = load_train_data()\n",
        "    \n",
        "    # calculate batch size\n",
        "    num_batch = len(X) // batch_size\n",
        "    \n",
        "    X = tf.convert_to_tensor(X, tf.int32)\n",
        "    Y = tf.convert_to_tensor(Y, tf.int32)\n",
        "    \n",
        "    input_queues = tf.compat.v1.train.slice_input_producer([X, Y])\n",
        "            \n",
        "    x, y = tf.compat.v1.train.shuffle_batch(input_queues,\n",
        "                                num_threads=8,\n",
        "                                batch_size=batch_size, \n",
        "                                capacity=batch_size*64,   \n",
        "                                min_after_dequeue=batch_size*32, \n",
        "                                allow_smaller_final_batch=False)\n",
        "    \n",
        "    return x, y, num_batch # (N, T), (N, T), ()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR-R9KTFz6EH"
      },
      "source": [
        "# layer normalization\n",
        "def normalize(inputs, epsilon = 1e-8, scope=\"ln\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        inputs_shape = inputs.get_shape()\n",
        "        params_shape = inputs_shape[-1:]\n",
        "    \n",
        "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "        beta= tf.Variable(tf.zeros(params_shape))\n",
        "        gamma = tf.Variable(tf.ones(params_shape))\n",
        "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
        "        outputs = gamma * normalized + beta\n",
        "        \n",
        "    return outputs\n",
        "\n",
        "# initial embedding operation\n",
        "def embedding(inputs, vocab_size, num_units, zero_pad=True, scale=True,scope=\"embedding\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        lookup_table = tf.get_variable('lookup_table',\n",
        "                                       dtype=tf.float32,\n",
        "                                       shape=[vocab_size, num_units],\n",
        "                                       initializer=tf.keras.initializers.glorot_normal()) \n",
        "        if zero_pad:\n",
        "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
        "                                      lookup_table[1:, :]), 0)\n",
        "        outputs = tf.nn.embedding_lookup(lookup_table, inputs) # complete word embedding, change into 3d tensor\n",
        "        \n",
        "        if scale:\n",
        "            outputs = outputs * (num_units ** 0.5) # scale\n",
        "            \n",
        "    return outputs\n",
        "    \n",
        "# positional encoding\n",
        "def positional_encoding(inputs, num_units,zero_pad=True, scale=True, scope=\"positional_encoding\", reuse=None):\n",
        "\n",
        "    N, T = inputs.get_shape().as_list()\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        \n",
        "        #position indices\n",
        "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n",
        "\n",
        "        position_enc = np.array([\n",
        "            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]\n",
        "            for pos in range(T)])\n",
        "\n",
        "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  \n",
        "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  \n",
        "\n",
        "        lookup_table = tf.convert_to_tensor(position_enc)\n",
        "\n",
        "        if zero_pad:\n",
        "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
        "                                      lookup_table[1:, :]), 0)\n",
        "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
        "\n",
        "        if scale:\n",
        "            outputs = outputs * num_units**0.5\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# attention mechanism\n",
        "def multihead_attention(queries, keys, num_units=None, num_heads=8, dropout_rate=0, is_training=True, causality=False, scope=\"multihead_attention\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Set the fall back option for num_units\n",
        "        if num_units is None:\n",
        "            num_units = queries.get_shape().as_list[-1]\n",
        "        \n",
        "        # Linear projections\n",
        "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
        "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        \n",
        "        # Split and concat\n",
        "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
        "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "\n",
        "        # Multiplication\n",
        "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
        "        \n",
        "        # Scale\n",
        "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "        \n",
        "        # Key Masking\n",
        "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
        "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
        "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
        "        \n",
        "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
        "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Causality = Future blinding\n",
        "        if causality:\n",
        "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
        "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
        "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
        "   \n",
        "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
        "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Activation\n",
        "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
        "         \n",
        "        # Query Masking\n",
        "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
        "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
        "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
        "        outputs *= query_masks # broadcasting. (h*N, T_q, T_k)\n",
        "          \n",
        "        # Dropouts avoid overfitting\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "               \n",
        "        # Weighted sum\n",
        "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
        "        \n",
        "        # Restore shape\n",
        "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
        "              \n",
        "        # Residual connection\n",
        "        outputs += queries\n",
        "              \n",
        "        # Normalize\n",
        "        outputs = normalize(outputs) # (N, T_q, C)\n",
        " \n",
        "    return outputs\n",
        "\n",
        "\n",
        "def feedforward(inputs, num_units=[2048, 512],scope=\"multihead_attention\", reuse=None):\n",
        "\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Inner layer\n",
        "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
        "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        \n",
        "        # Readout layer\n",
        "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
        "                  \"activation\": None, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        \n",
        "        # Residual connection\n",
        "        outputs += inputs\n",
        "        \n",
        "        # Normalize\n",
        "        outputs = normalize(outputs)\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "# smoothing\n",
        "def label_smoothing(inputs, epsilon=0.1):\n",
        "    ''' https://arxiv.org/abs/1512.00567.'''\n",
        "\n",
        "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
        "    return ((1-epsilon) * inputs) + (epsilon / K)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFf7vJY20FqC"
      },
      "source": [
        "class Graph():\n",
        "    def __init__(self, is_training=True):\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "            if is_training:\n",
        "                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n",
        "            else: # inference\n",
        "                self.x = tf.placeholder(tf.int32, shape=(None, maxlen))\n",
        "                self.y = tf.placeholder(tf.int32, shape=(None, maxlen))\n",
        "\n",
        "            # decoder input\n",
        "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
        "\n",
        "            # load   \n",
        "            cn2idx, idx2cn = load_cn_vocab()\n",
        "            en2idx, idx2en = load_en_vocab()\n",
        "            \n",
        "            # Encoder\n",
        "            with tf.variable_scope(\"encoder\"):\n",
        "                ## embedding\n",
        "                self.enc = embedding(self.x, \n",
        "                                      vocab_size=len(en2idx), \n",
        "                                      num_units=hidden_units, \n",
        "                                      scale=True,\n",
        "                                      scope=\"enc_embed\")\n",
        "                \n",
        "                ## positional embedding\n",
        "                if sinusoid:\n",
        "                    self.enc += positional_encoding(self.x,\n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"enc_pe\")\n",
        "                else:\n",
        "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"enc_pe\")\n",
        "                    \n",
        "                 \n",
        "                ## dropout layer\n",
        "                self.enc = tf.layers.dropout(self.enc, \n",
        "                                            rate=dropout_rate, \n",
        "                                            training=tf.convert_to_tensor(is_training))\n",
        "                \n",
        "                ## number of encoders: num_blocks\n",
        "                for i in range(num_blocks):\n",
        "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
        "                        ### Multihead Attention\n",
        "                        self.enc = multihead_attention(queries=self.enc, \n",
        "                                                        keys=self.enc, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads, \n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training,\n",
        "                                                        causality=False)\n",
        "                        \n",
        "                        ### Feed Forward\n",
        "                        self.enc = feedforward(self.enc, num_units=[4*hidden_units, hidden_units])\n",
        "            \n",
        "            # decoder\n",
        "            with tf.variable_scope(\"decoder\"):\n",
        "                self.dec = embedding(self.decoder_inputs, \n",
        "                                      vocab_size=len(cn2idx), \n",
        "                                      num_units=hidden_units,\n",
        "                                      scale=True, \n",
        "                                      scope=\"dec_embed\")\n",
        "                \n",
        "                if sinusoid:\n",
        "                    self.dec += positional_encoding(self.decoder_inputs,\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"dec_pe\")\n",
        "                else:\n",
        "                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
        "                                      vocab_size=maxlen, \n",
        "                                      num_units=hidden_units, \n",
        "                                      zero_pad=False, \n",
        "                                      scale=False,\n",
        "                                      scope=\"dec_pe\")\n",
        "                \n",
        "                self.dec = tf.layers.dropout(self.dec, \n",
        "                                            rate=dropout_rate, \n",
        "                                            training=tf.convert_to_tensor(is_training))\n",
        "                \n",
        "                for i in range(num_blocks):\n",
        "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
        "                        ## Multihead Attention ( self-attention)\n",
        "                        self.dec = multihead_attention(queries=self.dec, \n",
        "                                                        keys=self.dec, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads, \n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training,\n",
        "                                                        causality=True, \n",
        "                                                        scope=\"self_attention\")\n",
        "                        \n",
        "                        ## Multihead Attention (vanilla attention)\n",
        "                        self.dec = multihead_attention(queries=self.dec, \n",
        "                                                        keys=self.enc, \n",
        "                                                        num_units=hidden_units, \n",
        "                                                        num_heads=num_heads,\n",
        "                                                        dropout_rate=dropout_rate,\n",
        "                                                        is_training=is_training, \n",
        "                                                        causality=False,\n",
        "                                                        scope=\"vanilla_attention\")\n",
        "                        \n",
        "                        ## Feed Forward\n",
        "                        self.dec = feedforward(self.dec, num_units=[4*hidden_units, hidden_units])\n",
        "                \n",
        "            # Final linear projection\n",
        "            self.logits = tf.layers.dense(self.dec, len(cn2idx)) #shape: [N,T,len(en2idx)]\n",
        "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
        "            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
        "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
        "            tf.summary.scalar('acc', self.acc)\n",
        "            \n",
        "            if is_training:  \n",
        "                # smoothing\n",
        "                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(cn2idx)))\n",
        "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
        "                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
        "               \n",
        "                # graph\n",
        "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
        "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
        "                   \n",
        "                # summary\n",
        "                tf.summary.scalar('mean_loss', self.mean_loss)\n",
        "                self.merged = tf.summary.merge_all()\n",
        "\n",
        "# if __name__ == '__main__':                    \n",
        "#     cn2idx, idx2cn = load_cn_vocab()\n",
        "#     en2idx, idx2en = load_en_vocab()\n",
        "    \n",
        "#     g = Graph(\"train\"); print(\"load successfully\")\n",
        "    \n",
        "#     sv = tf.train.Supervisor(graph=g.graph, \n",
        "#                              logdir=logdir,\n",
        "#                              save_model_secs=0)\n",
        "#     with sv.managed_session() as sess:\n",
        "#         for epoch in range(1, num_epochs+1): \n",
        "#             print('The%d' % (epoch)+'period')\n",
        "#             if sv.should_stop(): break\n",
        "#             for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
        "#                 sess.run(g.train_op)\n",
        "                \n",
        "#             gs = sess.run(g.global_step)   \n",
        "#             sv.saver.save(sess, logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
        "    \n",
        "#     print(\"Done\")    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-sOd8y-4C2U",
        "outputId": "f75e69cd-dae0-461c-b825-d9ae9c5a312e"
      },
      "source": [
        "import os\n",
        "def eval(): \n",
        "    # load graph\n",
        "    g = Graph(is_training=False)\n",
        "    print(\"Graph loaded\")\n",
        "    \n",
        "    # load data\n",
        "    X, Sources, Targets = load_test_data()\n",
        "    cn2idx, idx2cn = load_cn_vocab()\n",
        "    en2idx, idx2en = load_en_vocab()\n",
        "     \n",
        "     \n",
        "    # Start session \n",
        "    with g.graph.as_default():    \n",
        "        sv = tf.train.Supervisor()\n",
        "        with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "            ## restore parameters\n",
        "            sv.saver.restore(sess, tf.train.latest_checkpoint(logdir))\n",
        "            print(\"Restored!\")\n",
        "              \n",
        "            ## Get model name\n",
        "            mname = open(logdir + '/checkpoint', 'r').read().split('\"')[1] # model name\n",
        "             \n",
        "            ## Inference\n",
        "            if not os.path.exists('drive/Shareddrives/ruochen.katherina@gmail.com/results_en_zh'): os.mkdir('drive/Shareddrives/ruochen.katherina@gmail.com/results_en_zh')\n",
        "            with codecs.open(\"drive/Shareddrives/ruochen.katherina@gmail.com/results_en_zh/\" + mname, \"w\", \"utf-8\") as fout:\n",
        "                list_of_refs, hypotheses = [], []\n",
        "                for i in range(len(X) // batch_size):\n",
        "                     \n",
        "                    ### Get mini-batches\n",
        "                    x = X[i*batch_size: (i+1)*batch_size]\n",
        "                    sources = Sources[i*batch_size: (i+1)*batch_size]\n",
        "                    targets = Targets[i*batch_size: (i+1)*batch_size]\n",
        "                     \n",
        "                    ### Autoregressive inference\n",
        "                    preds = np.zeros((batch_size, maxlen), np.int32)\n",
        "                    for j in range(maxlen):\n",
        "                        _preds = sess.run(g.preds, {g.x: x, g.y: preds})\n",
        "                        preds[:, j] = _preds[:, j]\n",
        "                     \n",
        "                    ### Write to file\n",
        "                    for source, target, pred in zip(sources, targets, preds): # sentence-wise\n",
        "                        got = \" \".join(idx2cn[idx] for idx in pred).split(\"</S>\")[0].strip()\n",
        "                        fout.write(\"- source: \" + source +\"\\n\")\n",
        "                        fout.write(\"- expected: \" + target + \"\\n\")\n",
        "                        fout.write(\"- got: \" + got + \"\\n\\n\")\n",
        "                        print(\"- source: \" + source +\"\\n\")\n",
        "                        print(\"- expected: \" + target + \"\\n\")\n",
        "                        print(\"- got: \" + got + \"\\n\\n\")\n",
        "                        fout.flush()\n",
        "                          \n",
        "                        #bleu score\n",
        "                        ref = target.split()\n",
        "                        hypothesis = got.split()\n",
        "                        if len(ref) > 3 and len(hypothesis) > 3:\n",
        "                            list_of_refs.append([ref])\n",
        "                            hypotheses.append(hypothesis)\n",
        "              \n",
        "                ## Calculate bleu score\n",
        "                print(list_of_refs)\n",
        "                score = corpus_bleu(list_of_refs, hypotheses)\n",
        "                fout.write(\"Bleu Score = \" + str(100*score))\n",
        "                print(\"Bleu Score = \" + str(100*score))\n",
        "                                          \n",
        "if __name__ == '__main__':\n",
        "    eval()\n",
        "    print(\"Done\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:268: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  warnings.warn('`tf.layers.dropout` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:202: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  warnings.warn('`tf.layers.conv1d` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Graph loaded\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Restoring parameters from drive/Shareddrives/ruochen.katherina@gmail.com/logdir_en_zh/model_epoch_20_gs_2640\n",
            "Restored!\n",
            "- source: he who seeks \" taiwan independence \" will come to no good end .\n",
            "\n",
            "- expected: 谁 搞 \" 台独 \" , 谁 就 没有 好下场 .\n",
            "\n",
            "- got: 谁 搞 \" 台独 \" , 谁 就 没有 <UNK> .\n",
            "\n",
            "\n",
            "- source: qian qichen hoped the people of all sectors in hong kong will make positive contribution to strengthening cross - strait cooperation and exchanges in various fields .\n",
            "\n",
            "- expected: 钱其琛 希望 香港 各界人士 在 加强 两岸 各 领域 合作 与 交流 方面 做出 积极 的 贡献 .\n",
            "\n",
            "- got: 钱其琛 希望 香港 <UNK> 在 加强 两岸 各 领域 合作 与 交流 方面 做出 积极 的 贡献 .\n",
            "\n",
            "\n",
            "- source: the dprk guests arrived china for a visit at the invitation of the cyl central committee .\n",
            "\n",
            "- expected: 朝鲜 客人 是 应 共青团中央 的 邀请 来华访问 的 .\n",
            "\n",
            "- got: 朝鲜 <UNK> 是 应 <UNK> 的 邀请 , 开始 <UNK> 的 .\n",
            "\n",
            "\n",
            "- source: the two sides exchanged views on a wide range of issues .\n",
            "\n",
            "- expected: 双方 就 广泛 问题 交换 了 看法 .\n",
            "\n",
            "- got: 双方 就 广泛 问题 交换 了 看法 .\n",
            "\n",
            "\n",
            "- source: in answering a question on the middle east peace process , sun yuxi said : the palestine issue is the core of the middle east issue .\n",
            "\n",
            "- expected: 在 回答 记者 有关 中东 和平 进程 问题 时 , 孙玉玺 说 , 巴勒斯坦 问题 是 中东问题 的 核心 .\n",
            "\n",
            "- got: 在 孙玉玺 <UNK> 问题 上 , 孙玉玺 说 , <UNK> 和平 进程 .\n",
            "\n",
            "\n",
            "- source: foreign minister tang jiaxuan held talks with him .\n",
            "\n",
            "- expected: 外交部长 唐家璇 同 他 举行 了 会谈 .\n",
            "\n",
            "- got: <UNK> 唐家璇 同 他 举行 了 会谈 .\n",
            "\n",
            "\n",
            "- source: today , central foreign affairs office director liu huaqiu met with him .\n",
            "\n",
            "- expected: 今天 , 中央 外事办公室 主任 刘华秋 会见 了 他 .\n",
            "\n",
            "- got: 今天 , 中央 <UNK> 主任 <UNK> 会见 了 他 .\n",
            "\n",
            "\n",
            "- source: this afternoon , president jiang zemin will meet with him .\n",
            "\n",
            "- expected: 今天下午 江泽民 主席 将 会见 他 .\n",
            "\n",
            "- got: 今天下午 江泽民 主席 将 会见 他 .\n",
            "\n",
            "\n",
            "- source: to this , berger said the relations between the united states and china are at a crucial moment .\n",
            "\n",
            "- expected: 对此 , 伯杰 表示 , 美国 和 中国 的 关系 正处 於 关键时刻 .\n",
            "\n",
            "- got: 对此 , <UNK> 表示 , 美国 和 中国 的 关系 <UNK> 於 <UNK> .\n",
            "\n",
            "\n",
            "- source: li lanqing , member of the cpc politburo standing committee and vice premier of the state council , attended the conference and delivered a speech .\n",
            "\n",
            "- expected: 中共 中央政治局常委 , 国务院 副 总理 李岚清 出席会议 并 讲话 .\n",
            "\n",
            "- got: 中共 <UNK> , 国务院 副 总理 李岚清 <UNK> 并 讲话 .\n",
            "\n",
            "\n",
            "- source: as a major survey on the national conditions and national strength , the census is a huge social systems engineering .\n",
            "\n",
            "- expected: 人口普查 作为 重大 的 国情 国力 调查 , 是 一项 庞大 的 社会 系统工程 .\n",
            "\n",
            "- got: <UNK> 作为 重大 的 <UNK> <UNK> <UNK> , 是 一项 <UNK> 的 社会 系统工程 .\n",
            "\n",
            "\n",
            "- source: in order to do a good job of the census , the state council and pertinent departments have drawn up relevant supporting policies .\n",
            "\n",
            "- expected: 为了 搞好 普查 , 国务院 有关 部门 已 制定 了 相关 配套 政策 , 各 地区 要 认真 贯彻落实 , 不能 自 定 政策 , 各行其是 .\n",
            "\n",
            "- got: 为了 搞好 <UNK> , 国务院 有关 部门 已 制定 了 <UNK> <UNK> 政策 , 各 地区 要 认真 贯彻落实 , 各 地区 要 认真 贯彻落实 , <UNK> .\n",
            "\n",
            "\n",
            "- source: the local governments which have not made arrangements or have not made adequate arrangements for the funds must carry out the work in no time .\n",
            "\n",
            "- expected: 地方 政府 的 经费 还 没有 安排 或者 安排 得 不足 的 , 要 抓紧 落实 .\n",
            "\n",
            "- got: 地方 政府 的 <UNK> 还 没有 安排 得 不足 的 , 要 抓紧 落实 .\n",
            "\n",
            "\n",
            "- source: all citizens should fulfill their own obligations and report the relevant conditions and data accurately .\n",
            "\n",
            "- expected: 每个 公民 都 要 认真 履行 自己 应尽 的 义务 , 如实 申报 有关 情况 和 数据 .\n",
            "\n",
            "- got: <UNK> 义务 , <UNK> 代表 们 都 要 认真 履行 自己 <UNK> 的 义务 .\n",
            "\n",
            "\n",
            "- source: he stressed that governments at all levels should effectively carry out various tasks of the census .\n",
            "\n",
            "- expected: 他 强调 , 各级 政府 要 把 人口普查 各项 工作 落到实处 .\n",
            "\n",
            "- got: 他 强调 , 各级 政府 要 把 <UNK> 各项 工作 <UNK> .\n",
            "\n",
            "\n",
            "- source: sun yuxi solemnly pointed out : the taiwan issue is completely an internal issue of china .\n",
            "\n",
            "- expected: 孙玉玺 严正 指出 , 台湾 问题 纯属 中国 内政 .\n",
            "\n",
            "- got: 孙玉玺 <UNK> 指出 , 台湾 问题 <UNK> 中国 内政 .\n",
            "\n",
            "\n",
            "- source: the japanese government clearly and solemnly stated to china on various occasions its stand on the taiwan issue and made a series of solemn commitments .\n",
            "\n",
            "- expected: 日本政府 曾 就 台湾 问题 向 中国 作出 过 一系列 郑重 表态 和 承诺 .\n",
            "\n",
            "- got: <UNK> <UNK> 曾 就 台湾 问题 向 中国 作出 过 一系列 <UNK> <UNK> <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: china resolutely opposes any countries which have diplomatic relations with china establishing official relations with taiwan or engaging in exchanges of any kind of official nature with taiwan .\n",
            "\n",
            "- expected: 中国 坚决 反对 任何 与 中国 建交国 , 同 台湾 建立 官方 关系 或 进行 具有 任何 性质 的 官方 往来 .\n",
            "\n",
            "- got: 中国 坚决 反对 任何 与 中国 <UNK> , 同 中国 <UNK> 关系 或 进行 具有 任何 性质 的 <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: wen jiabao pointed out the need to gain a full understanding of the important significance of land greening .\n",
            "\n",
            "- expected: 温家宝 指出 , 要 充分认识 国土 绿化 的 重要 意义 .\n",
            "\n",
            "- got: 温家宝 指出 , 要 <UNK> 绿化 的 重要 意义 .\n",
            "\n",
            "\n",
            "- source: china has made tremendous achievements in greening its land and last year , afforestation and greening maintained a positive momentum of development .\n",
            "\n",
            "- expected: 我国 国土 绿化 事业 取得 了 巨大成就 , 去年 造林 绿化 继续 保持 了 良好 的 发展势头 .\n",
            "\n",
            "- got: 我国 <UNK> 绿化 事业 取得 了 <UNK> , 去年 <UNK> 绿化 继续 保持 了 良好 的 <UNK> .\n",
            "\n",
            "\n",
            "- source: but we must also be soberly aware that the situation concerning china 's ecological environment remains rather grim and water losses and soil erosion are serious .\n",
            "\n",
            "- expected: 但 也 必须 清醒 看到 , 我国 的 生态环境 状况 依然 相当 严峻 , 水土流失 严重 .\n",
            "\n",
            "- got: 但 也 必须 清醒 看到 , 我国 的 生态环境 状况 <UNK> 相当 <UNK> , <UNK> 严重 .\n",
            "\n",
            "\n",
            "- source: the main tasks of afforestation at present are manifold . first , the all - people voluntary tree - planting campaign should be continued .\n",
            "\n",
            "- expected: 当前 国土 绿化 工作 的 主要 任务 , 一是 继续 开展 全民 义务 植树 运动 .\n",
            "\n",
            "- got: 当前 <UNK> 绿化 工作 的 主要 任务 , 一是 继续 开展 <UNK> 义务 , <UNK> <UNK> <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: second , the urban and rural greening process should be comprehensively accelerated .\n",
            "\n",
            "- expected: 坚持 以城带 乡 , 以乡 促城 , 城乡 联动 , 总体 推进 .\n",
            "\n",
            "- got: 坚持 <UNK> , <UNK> , <UNK> , 总体 推进 .\n",
            "\n",
            "\n",
            "- source: third , conditions should be created for the improvement of conditions for agricultural production and the ecological environment and for the realization of sustainable agricultural development .\n",
            "\n",
            "- expected: 三是 为 改善 农业 生产 条件 和 生态环境 服务 , 为 实现 农业 可 持续 发展 创造条件 .\n",
            "\n",
            "- got: 三是 为 改善 农业 生产 条件 和 生态环境 服务 , 为 实现 农业 可 持续 发展 <UNK> .\n",
            "\n",
            "\n",
            "- source: fourth , this process should be integrated with the strategy of developing the western country in a big way .\n",
            "\n",
            "- expected: 四是 与 实施 西部 大 开发 战略 结合 起来 .\n",
            "\n",
            "- got: <UNK> 与 实施 西部 大 开发 战略 结合 起来 .\n",
            "\n",
            "\n",
            "- source: all departments concerned should give energetic support to relevant industries in the greening campaign .\n",
            "\n",
            "- expected: 各 有关 部门 要 积极开展 绿化 对口 支援 活动 .\n",
            "\n",
            "- got: 各 有关 部门 要 <UNK> 绿化 <UNK> 活动 .\n",
            "\n",
            "\n",
            "- source: second , it is necessary to rely on scientific and technological advancement to raise the standard of land greening .\n",
            "\n",
            "- expected: 二要 依靠 科技进步 , 提高 国土 绿化 水平 .\n",
            "\n",
            "- got: <UNK> 依靠 科技进步 , 提高 <UNK> 绿化 水平 .\n",
            "\n",
            "\n",
            "- source: third , it is necessary to perfect the mechanisms and arouse the general public 's enthusiasm in afforestation .\n",
            "\n",
            "- expected: 三要 完善 机制 , 调动 广大群众 的 绿化 积极性 .\n",
            "\n",
            "- got: <UNK> 完善 机制 , 调动 <UNK> 的 绿化 积极性 .\n",
            "\n",
            "\n",
            "- source: fourth , it is necessary to manage forests in strict accordance with law and comprehensively strengthen the protection of forest and pasture resources and the fruits of afforestation .\n",
            "\n",
            "- expected: 四要 严格 依法 治林 , 全面 加强 林草 资源 与 绿化 成果 的 保护 .\n",
            "\n",
            "- got: <UNK> 严格 依法 <UNK> , 全面 加强 <UNK> 资源 与 绿化 成果 的 保护 .\n",
            "\n",
            "\n",
            "- source: new progress should be made in legislation on land greening and new measures introduced to intensify law enforcement .\n",
            "\n",
            "- expected: 要 在 国土 绿化 立法 上 取得 新进展 , 在 加大 执法 力度 上 有 新举措 .\n",
            "\n",
            "- got: 要 在 <UNK> 绿化 立法 上 取得 <UNK> , 在 加大 执法 力度 上 有 <UNK> .\n",
            "\n",
            "\n",
            "- source: fifth , it is necessary to unify people 's understanding and conscientiously strengthen leadership over greening work .\n",
            "\n",
            "- expected: 五要 统一认识 , 切实加强 对 绿化 工作 的 领导 .\n",
            "\n",
            "- got: <UNK> <UNK> , 切实加强 对 绿化 工作 的 领导 .\n",
            "\n",
            "\n",
            "- source: following is the full text of the speech .\n",
            "\n",
            "- expected: 会议 上 发言\n",
            "\n",
            "- got: 会议 上 发言\n",
            "\n",
            "\n",
            "- source: i am extremely glad to meet my old friends again and to make new friends here .\n",
            "\n",
            "- expected: 能 在 这里 重逢 故友 , 结交 新朋 , 我 感到 格外 高兴 .\n",
            "\n",
            "- got: 我 想 <UNK> <UNK> <UNK> , 我 感到 <UNK> <UNK> <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: i want to express my heartfelt thanks to the host for its cordial invitation and warm reception !\n",
            "\n",
            "- expected: 我 对 东道主 的 盛情 邀请 和 热情接待 表示 衷心 的 感谢 !\n",
            "\n",
            "- got: 我 对 <UNK> 的 <UNK> 邀请 和 <UNK> 表示 <UNK> 的 <UNK> !\n",
            "\n",
            "\n",
            "- source: after the two agreements came into force , the five countries have done a positive and fruitful work to honor the agreements .\n",
            "\n",
            "- expected: 两个 协定 生效 后 , 各国 为 履约 进行 了 积极 而 富有成效 的 工作 .\n",
            "\n",
            "- got: 两个 <UNK> 后 , 各国 为 <UNK> 进行 了 积极 而 <UNK> 的 工作 .\n",
            "\n",
            "\n",
            "- source: we believe that all the five countries will strictly abide by the agreements without violating the stipulations .\n",
            "\n",
            "- expected: 我们 认为 , 各方 都 能 严格遵守 协定 , 没有 任何 违反 协定 规定 的 行为 .\n",
            "\n",
            "- got: 我们 认为 , 各方 都 能 <UNK> , 没有 任何 违反 <UNK> 规定 的 行为 .\n",
            "\n",
            "\n",
            "- source: as far as i know , china and russia will conduct another test examination in suifenhe in china 's heilongjiang province in mid - april .\n",
            "\n",
            "- expected: 据我所知 , 中 俄 双方 还 将 于 4 月 中旬 在 中国 黑龙江省 绥芬河 再次 进行 试验性 核查 .\n",
            "\n",
            "- got: <UNK> , 中 俄 双方 还 将 于 4 月 <UNK> 在 中国 <UNK> 再次 进行 <UNK> <UNK> <UNK> <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: the cooperation based on mutual benefit and the common development are the economic guarantee for safeguarding peace and security .\n",
            "\n",
            "- expected: 互利 合作 , 共同 发展 , 是 维护和平 与 安全 的 经济 保障 .\n",
            "\n",
            "- got: <UNK> 合作 , 共同 发展 , 是 <UNK> 与 安全 的 经济 保障 .\n",
            "\n",
            "\n",
            "- source: and , holding dialogues , negotiations , and talks based on equality is the correct channel to solve disputes and safeguard peace and security .\n",
            "\n",
            "- expected: 建立 在 平等 基础 上 的 对话 , 协商 和 谈判 , 是 解决 争端 , 维护和平 与 安全 的 正确 途径 .\n",
            "\n",
            "- got: 建立 在 平等 基础 上 的 对话 , 协商 和 谈判 , 协商 讨论 , <UNK> , <UNK> 与 安全 的 正确 <UNK> .\n",
            "\n",
            "\n",
            "- source: we support russia 's resolute measures against chechen and support russian government 's sacred cause of safeguarding national unification and territorial integrity .\n",
            "\n",
            "- expected: 我们 支持 俄罗斯 在 车臣 问题 上 采取 的 果断 措施 , 支持 俄罗斯政府 维护 国家 统一 和 领土完整 的 神圣 事业 .\n",
            "\n",
            "- got: 我们 支持 俄罗斯 在 车臣 问题 上 采取 的 <UNK> 措施 , 支持 <UNK> 维护 国家 统一 和 领土完整 的 神圣 事业 .\n",
            "\n",
            "\n",
            "- source: we also support the efforts made by the middle east countries to safeguard the national and regional peace and stability .\n",
            "\n",
            "- expected: 我们 也 同样 支持 中亚国家 为 维护 本国 和 地区 和平 与 稳定 所作 的 努力 .\n",
            "\n",
            "- got: 我们 也 <UNK> 支持 <UNK> 为 维护 <UNK> 和 地区 和平 与 稳定 所作 的 努力 .\n",
            "\n",
            "\n",
            "- source: as defense ministers , we are duty - bound to carry this consensus out .\n",
            "\n",
            "- expected: 作为 国防部长 , 我们 有 责任 落实 这一 共识 .\n",
            "\n",
            "- got: 作为 国防部长 , 我们 有 责任 落实 这一 共识 .\n",
            "\n",
            "\n",
            "- source: our refusal to give up the use of force is not directed against taiwan people but is directed against foreign forces and taiwan independence forces .\n",
            "\n",
            "- expected: 我们 不 承诺 放弃 使用 武力 , 不是 针对 台湾 人民 , 而是 针对 外国 干涉 势力 和 台独 势力 .\n",
            "\n",
            "- got: 我们 不 承诺 放弃 使用 武力 , 不是 针对 台湾 人民 , 而是 针对 台湾 人民 .\n",
            "\n",
            "\n",
            "- source: thank you !\n",
            "\n",
            "- expected: 谢谢 各位 !\n",
            "\n",
            "- got: <UNK> <UNK> !\n",
            "\n",
            "\n",
            "- source: they should step up their studies , and constantly enhance their work ability and work level .\n",
            "\n",
            "- expected: 要 加强 学习 , 不断 提高 自己 的 工作 能力 和 水平 .\n",
            "\n",
            "- got: 要 加强 学习 , 不断 提高 自己 的 工作 能力 和 水平 .\n",
            "\n",
            "\n",
            "- source: the meeting was presided over by wang ruixiang , deputy secretary of the central work committee for enterprises .\n",
            "\n",
            "- expected: 中央 企业 工委 副 书记 王瑞祥 主持会议 .\n",
            "\n",
            "- got: 中央 企业 <UNK> 副 书记 <UNK> <UNK> .\n",
            "\n",
            "\n",
            "- source: to improve the ecological environment is a task of prime importance in the large - scale development of the western region .\n",
            "\n",
            "- expected: 改善 生态环境 是 西部 大 开发 的 重中之重 .\n",
            "\n",
            "- got: 改善 生态环境 是 西部 大 开发 的 <UNK> .\n",
            "\n",
            "\n",
            "- source: chi haotian said : china and tajikistan are friendly neighboring countries , and peoples of the two countries have their traditional friendship .\n",
            "\n",
            "- expected: 迟浩田 说 , 中塔 是 友好邻邦 , 两国人民 有着 传统友谊 .\n",
            "\n",
            "- got: 迟浩田 说 , <UNK> , 两国人民 有着 <UNK> , <UNK> .\n",
            "\n",
            "\n",
            "- source: since the establishment of diplomatic relations between the two countries , their bilateral ties have been developing steadily , and their fields of cooperation have been constantly expanded .\n",
            "\n",
            "- expected: 两 国 建交 以来 , 双边关系 稳步发展 , 合作 领域 不断扩大 .\n",
            "\n",
            "- got: 两 国 建交 以来 , <UNK> , 合作 领域 <UNK> .\n",
            "\n",
            "\n",
            "- source: khairullaev said : president rakhmonov and president jiang zemin have laid a solid foundation for the development of the ties between the two countries .\n",
            "\n",
            "- expected: 海鲁洛耶夫 说 , 拉赫莫诺夫 总统 和 江泽民 主席 为 两国关系 的 发展 奠定 了 良好 的 基础 .\n",
            "\n",
            "- got: <UNK> 说 , 江泽民 主席 为 两国关系 的 发展 <UNK> 了 良好 的 基础 .\n",
            "\n",
            "\n",
            "- source: in the future , tajikistan will continue to make efforts to safeguard its own and regional security and stability .\n",
            "\n",
            "- expected: 今后 塔 仍 将 为此 继续 努力 , 以 维护 本身 及 地区 的 安全 与 稳定 .\n",
            "\n",
            "- got: 今后 <UNK> 仍 将 为此 继续 努力 , 以 维护 <UNK> 及 地区 的 安全 与 稳定 .\n",
            "\n",
            "\n",
            "- source: we are also pleased to see that the current sino - indian relations have entered a new stage of improvement and development .\n",
            "\n",
            "- expected: 我们 还 高兴 地 看到 , 当前 中印关系 又 进入 了 一个 改善 和 发展 的 新 阶段 .\n",
            "\n",
            "- got: 我们 还 <UNK> 地 看到 , 当前 <UNK> 又 进入 了 一个 改善 和 发展 的 新 阶段 .\n",
            "\n",
            "\n",
            "- source: as the two largest developing countries in the world , china and india share a vast common understanding and conduct much good cooperation in handling international affairs .\n",
            "\n",
            "- expected: 作为 世界 上 两 最大 的 发展中国家 , 我们 在 国际事务 中 有着 广泛 的 共识 和 诸多 良好 的 配合 .\n",
            "\n",
            "- got: 作为 世界 上 两 最大 的 发展中国家 , 我们 在 <UNK> 中 有着 广泛 的 共识 和 <UNK> 良好 的 <UNK> .\n",
            "\n",
            "\n",
            "- source: india is one of the first countries to establish diplomatic ties with china following the founding of new china .\n",
            "\n",
            "- expected: 新 中国 成立 后 , 印度 是 率先 同 中国 建交 的 国家 之一 .\n",
            "\n",
            "- got: 新 中国 成立 后 , 印度 是 <UNK> 同 中国 建交 的 国家 之一 .\n",
            "\n",
            "\n",
            "- source: with similar national situations , china and india share a vast common understanding and common interests , thus constituting a firm foundation for our friendship .\n",
            "\n",
            "- expected: 中印 两国 国情 相似 , 我们 之间 存在 着 广泛 的 共识 和 共同利益 , 这 构成 了 我们 友好相处 的 坚实基础 .\n",
            "\n",
            "- got: <UNK> 两国 <UNK> <UNK> , 我们 之间 存在 着 广泛 的 共识 和 <UNK> , 这 <UNK> 了 我们 <UNK> 的 共识 .\n",
            "\n",
            "\n",
            "- source: a bright prospect for developing sino - indian relations is now arising .\n",
            "\n",
            "- expected: 中印关系 正 展现出 美好 的 发展前景 .\n",
            "\n",
            "- got: <UNK> 正 <UNK> <UNK> 的 <UNK> .\n",
            "\n",
            "\n",
            "- source: the chinese leaders of three generations have paid attention to developing good - neighborly and friendly relations with india .\n",
            "\n",
            "- expected: 中国 三 领导人 都 重视 发展 与 印度 的 睦邻友好 关系 .\n",
            "\n",
            "- got: 中国 三 领导人 都 重视 发展 与 印度 的 <UNK> 关系 .\n",
            "\n",
            "\n",
            "- source: kazakh president nazarbayev met with the defense ministers of china , russia , kyrgyzstan , and tajikistan who were attending the defense minister meeting .\n",
            "\n",
            "- expected: 哈萨克斯坦 总统 纳扎尔巴耶夫 会见 了 参加 会晤 的 中 , 俄 , 吉 , 塔 四国 国防部长 .\n",
            "\n",
            "- got: <UNK> 总统 <UNK> 会见 了 参加 会晤 的 中 , <UNK> <UNK> <UNK> , <UNK> <UNK> 总统 .\n",
            "\n",
            "\n",
            "- source: also , both special care given to disabled people and orphans and conditions provided for their recuperation need further improvement .\n",
            "\n",
            "- expected: 同时 , 残疾人 和 孤儿 的 养护 , 康复 条件 也 需要 进一步 改善 .\n",
            "\n",
            "- got: 同时 , <UNK> 和 <UNK> 的 <UNK> , <UNK> 条件 也 需要 进一步 改善 .\n",
            "\n",
            "\n",
            "- source: all city people 's governments should reduce or exempt the urban infrastructure construction fee on these facilities as appropriate . 4 .\n",
            "\n",
            "- expected: 各市 人民政府 对 此项 市政 基础设施 配套 建设费 应 酌情 给予 减免 .\n",
            "\n",
            "- got: <UNK> <UNK> 对 <UNK> <UNK> 基础设施 <UNK> 应 <UNK> 给予 <UNK> .\n",
            "\n",
            "\n",
            "- source: for example , a high - tech estate should be set up in beijing 's zhongguancun , and urban planning should provide services for it .\n",
            "\n",
            "- expected: 如 北京 中关村 要 建高 科技园 , 城市规划 就 为 其 服务 , 提供 多个 规划 方案 进行 优化 比选 .\n",
            "\n",
            "- got: 如 北京 <UNK> 要 <UNK> , <UNK> 就 为 其 服务 , 提供 <UNK> 规划 方案 进行 优化 <UNK> .\n",
            "\n",
            "\n",
            "- source: we have the advantages of comparatively low wages and high - quality labor force . accession to the wto will greatly expand our market and tap our potential .\n",
            "\n",
            "- expected: 我们 有 工资 成本 较 低 , 素质 较 高 的 劳动力 , 将会 极大 地 释放 市场 拓展 潜力 .\n",
            "\n",
            "- got: 我们 有 <UNK> 成本 较 低 , 素质 较 高 的 <UNK> , 将会 极大 地 <UNK> 市场 <UNK> 市场 <UNK> .\n",
            "\n",
            "\n",
            "- source: the one - man camouflage smoke launcher and assembled false target that they developed and improved won a third - class prize for military scientific and technological progress .\n",
            "\n",
            "- expected: 他们 革新 成功 的 单兵 伪装 烟幕 发射装置 , 组装 式 假 目标 等 成果 , 还 获得 军队 科技进步 成果 三等奖 .\n",
            "\n",
            "- got: 他们 <UNK> 成功 的 <UNK> <UNK> <UNK> <UNK> , <UNK> <UNK> <UNK> 目标 等 成果 , 还 获得 军队 科技进步 成果 <UNK> .\n",
            "\n",
            "\n",
            "- source: does this mean that brown 's worries were without basis ?\n",
            "\n",
            "- expected: 这 是否 意味着 布朗 的 担忧 没有 根据 ?\n",
            "\n",
            "- got: 这 是否 <UNK> 的 <UNK> 没有 根据 ?\n",
            "\n",
            "\n",
            "[[['谁', '搞', '\"', '台独', '\"', ',', '谁', '就', '没有', '好下场', '.']], [['钱其琛', '希望', '香港', '各界人士', '在', '加强', '两岸', '各', '领域', '合作', '与', '交流', '方面', '做出', '积极', '的', '贡献', '.']], [['朝鲜', '客人', '是', '应', '共青团中央', '的', '邀请', '来华访问', '的', '.']], [['双方', '就', '广泛', '问题', '交换', '了', '看法', '.']], [['在', '回答', '记者', '有关', '中东', '和平', '进程', '问题', '时', ',', '孙玉玺', '说', ',', '巴勒斯坦', '问题', '是', '中东问题', '的', '核心', '.']], [['外交部长', '唐家璇', '同', '他', '举行', '了', '会谈', '.']], [['今天', ',', '中央', '外事办公室', '主任', '刘华秋', '会见', '了', '他', '.']], [['今天下午', '江泽民', '主席', '将', '会见', '他', '.']], [['对此', ',', '伯杰', '表示', ',', '美国', '和', '中国', '的', '关系', '正处', '於', '关键时刻', '.']], [['中共', '中央政治局常委', ',', '国务院', '副', '总理', '李岚清', '出席会议', '并', '讲话', '.']], [['人口普查', '作为', '重大', '的', '国情', '国力', '调查', ',', '是', '一项', '庞大', '的', '社会', '系统工程', '.']], [['为了', '搞好', '普查', ',', '国务院', '有关', '部门', '已', '制定', '了', '相关', '配套', '政策', ',', '各', '地区', '要', '认真', '贯彻落实', ',', '不能', '自', '定', '政策', ',', '各行其是', '.']], [['地方', '政府', '的', '经费', '还', '没有', '安排', '或者', '安排', '得', '不足', '的', ',', '要', '抓紧', '落实', '.']], [['每个', '公民', '都', '要', '认真', '履行', '自己', '应尽', '的', '义务', ',', '如实', '申报', '有关', '情况', '和', '数据', '.']], [['他', '强调', ',', '各级', '政府', '要', '把', '人口普查', '各项', '工作', '落到实处', '.']], [['孙玉玺', '严正', '指出', ',', '台湾', '问题', '纯属', '中国', '内政', '.']], [['日本政府', '曾', '就', '台湾', '问题', '向', '中国', '作出', '过', '一系列', '郑重', '表态', '和', '承诺', '.']], [['中国', '坚决', '反对', '任何', '与', '中国', '建交国', ',', '同', '台湾', '建立', '官方', '关系', '或', '进行', '具有', '任何', '性质', '的', '官方', '往来', '.']], [['温家宝', '指出', ',', '要', '充分认识', '国土', '绿化', '的', '重要', '意义', '.']], [['我国', '国土', '绿化', '事业', '取得', '了', '巨大成就', ',', '去年', '造林', '绿化', '继续', '保持', '了', '良好', '的', '发展势头', '.']], [['但', '也', '必须', '清醒', '看到', ',', '我国', '的', '生态环境', '状况', '依然', '相当', '严峻', ',', '水土流失', '严重', '.']], [['当前', '国土', '绿化', '工作', '的', '主要', '任务', ',', '一是', '继续', '开展', '全民', '义务', '植树', '运动', '.']], [['坚持', '以城带', '乡', ',', '以乡', '促城', ',', '城乡', '联动', ',', '总体', '推进', '.']], [['三是', '为', '改善', '农业', '生产', '条件', '和', '生态环境', '服务', ',', '为', '实现', '农业', '可', '持续', '发展', '创造条件', '.']], [['四是', '与', '实施', '西部', '大', '开发', '战略', '结合', '起来', '.']], [['各', '有关', '部门', '要', '积极开展', '绿化', '对口', '支援', '活动', '.']], [['二要', '依靠', '科技进步', ',', '提高', '国土', '绿化', '水平', '.']], [['三要', '完善', '机制', ',', '调动', '广大群众', '的', '绿化', '积极性', '.']], [['四要', '严格', '依法', '治林', ',', '全面', '加强', '林草', '资源', '与', '绿化', '成果', '的', '保护', '.']], [['要', '在', '国土', '绿化', '立法', '上', '取得', '新进展', ',', '在', '加大', '执法', '力度', '上', '有', '新举措', '.']], [['五要', '统一认识', ',', '切实加强', '对', '绿化', '工作', '的', '领导', '.']], [['能', '在', '这里', '重逢', '故友', ',', '结交', '新朋', ',', '我', '感到', '格外', '高兴', '.']], [['我', '对', '东道主', '的', '盛情', '邀请', '和', '热情接待', '表示', '衷心', '的', '感谢', '!']], [['两个', '协定', '生效', '后', ',', '各国', '为', '履约', '进行', '了', '积极', '而', '富有成效', '的', '工作', '.']], [['我们', '认为', ',', '各方', '都', '能', '严格遵守', '协定', ',', '没有', '任何', '违反', '协定', '规定', '的', '行为', '.']], [['据我所知', ',', '中', '俄', '双方', '还', '将', '于', '4', '月', '中旬', '在', '中国', '黑龙江省', '绥芬河', '再次', '进行', '试验性', '核查', '.']], [['互利', '合作', ',', '共同', '发展', ',', '是', '维护和平', '与', '安全', '的', '经济', '保障', '.']], [['建立', '在', '平等', '基础', '上', '的', '对话', ',', '协商', '和', '谈判', ',', '是', '解决', '争端', ',', '维护和平', '与', '安全', '的', '正确', '途径', '.']], [['我们', '支持', '俄罗斯', '在', '车臣', '问题', '上', '采取', '的', '果断', '措施', ',', '支持', '俄罗斯政府', '维护', '国家', '统一', '和', '领土完整', '的', '神圣', '事业', '.']], [['我们', '也', '同样', '支持', '中亚国家', '为', '维护', '本国', '和', '地区', '和平', '与', '稳定', '所作', '的', '努力', '.']], [['作为', '国防部长', ',', '我们', '有', '责任', '落实', '这一', '共识', '.']], [['我们', '不', '承诺', '放弃', '使用', '武力', ',', '不是', '针对', '台湾', '人民', ',', '而是', '针对', '外国', '干涉', '势力', '和', '台独', '势力', '.']], [['要', '加强', '学习', ',', '不断', '提高', '自己', '的', '工作', '能力', '和', '水平', '.']], [['中央', '企业', '工委', '副', '书记', '王瑞祥', '主持会议', '.']], [['改善', '生态环境', '是', '西部', '大', '开发', '的', '重中之重', '.']], [['迟浩田', '说', ',', '中塔', '是', '友好邻邦', ',', '两国人民', '有着', '传统友谊', '.']], [['两', '国', '建交', '以来', ',', '双边关系', '稳步发展', ',', '合作', '领域', '不断扩大', '.']], [['海鲁洛耶夫', '说', ',', '拉赫莫诺夫', '总统', '和', '江泽民', '主席', '为', '两国关系', '的', '发展', '奠定', '了', '良好', '的', '基础', '.']], [['今后', '塔', '仍', '将', '为此', '继续', '努力', ',', '以', '维护', '本身', '及', '地区', '的', '安全', '与', '稳定', '.']], [['我们', '还', '高兴', '地', '看到', ',', '当前', '中印关系', '又', '进入', '了', '一个', '改善', '和', '发展', '的', '新', '阶段', '.']], [['作为', '世界', '上', '两', '最大', '的', '发展中国家', ',', '我们', '在', '国际事务', '中', '有着', '广泛', '的', '共识', '和', '诸多', '良好', '的', '配合', '.']], [['新', '中国', '成立', '后', ',', '印度', '是', '率先', '同', '中国', '建交', '的', '国家', '之一', '.']], [['中印', '两国', '国情', '相似', ',', '我们', '之间', '存在', '着', '广泛', '的', '共识', '和', '共同利益', ',', '这', '构成', '了', '我们', '友好相处', '的', '坚实基础', '.']], [['中印关系', '正', '展现出', '美好', '的', '发展前景', '.']], [['中国', '三', '领导人', '都', '重视', '发展', '与', '印度', '的', '睦邻友好', '关系', '.']], [['哈萨克斯坦', '总统', '纳扎尔巴耶夫', '会见', '了', '参加', '会晤', '的', '中', ',', '俄', ',', '吉', ',', '塔', '四国', '国防部长', '.']], [['同时', ',', '残疾人', '和', '孤儿', '的', '养护', ',', '康复', '条件', '也', '需要', '进一步', '改善', '.']], [['各市', '人民政府', '对', '此项', '市政', '基础设施', '配套', '建设费', '应', '酌情', '给予', '减免', '.']], [['如', '北京', '中关村', '要', '建高', '科技园', ',', '城市规划', '就', '为', '其', '服务', ',', '提供', '多个', '规划', '方案', '进行', '优化', '比选', '.']], [['我们', '有', '工资', '成本', '较', '低', ',', '素质', '较', '高', '的', '劳动力', ',', '将会', '极大', '地', '释放', '市场', '拓展', '潜力', '.']], [['他们', '革新', '成功', '的', '单兵', '伪装', '烟幕', '发射装置', ',', '组装', '式', '假', '目标', '等', '成果', ',', '还', '获得', '军队', '科技进步', '成果', '三等奖', '.']], [['这', '是否', '意味着', '布朗', '的', '担忧', '没有', '根据', '?']]]\n",
            "Bleu Score = 53.722808682902944\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}