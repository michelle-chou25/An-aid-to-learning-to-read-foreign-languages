{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_attention.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11783wvSfhExEjYlp3dnxQkx243MMLUJW","authorship_tag":"ABX9TyOhCp4c5D0+cZ1n8BunjkCt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SueC4mk8-djb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789740605,"user_tz":-480,"elapsed":2623,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"5e310ec6-0548-4a2b-ea15-dea05065c619"},"source":["import matplotlib as mlp\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import sklearn\n","import pandas as pd\n","import os\n","# os._exit(00)\n","import sys\n","import time\n","import codecs\n","import tensorflow as tf\n","tf.compat.v1.enable_eager_execution()\n","from tensorflow import keras\n","import re\n","\n","# 1. preprocessing data\n","# 2. build model\n","# 2.1 encoder\n","# 2.2 attention\n","# 2.3 decoder\n","# 3. evaluation\n","# 3.1 given sentence, return translated results\n","\n","# hyperparameters\n","batch_size = 64\n","epochs = 20\n","embedding_units = 256\n","units = 1024\n","\n","# filename = 'drive/MyDrive/en_simple.txt'\n","# def preprocess_sentence(s):\n","#   s = s.strip()\n","#   s = '<s> ' + s + ' </s>'\n","#   return s\n","\n","# def parse_data(filename):\n","#   lines = open(filename, encoding = 'UTF-8').read().strip().split('\\n')\n","#   sentence_pairs = [line.split('\\t') for line in lines]\n","#   preprocessed_sentence_pairs = [(preprocess_sentence(en), preprocess_sentence(cn)) for en, cn in sentence_pairs]\n","#   return zip(*preprocessed_sentence_pairs)\n","\n","# target, source = parse_data(filename)\n","# print(en_dataset[-1])\n","# print(cn_dataset[-1])\n","\n","\n","## preprocessing data\n","source_path = \"drive/MyDrive/Corpus/chinese_out.txt\"\n","target_path = \"drive/MyDrive/Corpus/english1w.txt\"\n","\n","# add <S> and </S>\n","def MakeDataset(file_path):\n","    dataset = [(\"<S> \" + line + \" </S>\") for line in codecs.open(file_path, 'r', 'utf-8').read().splitlines()]\n","    return dataset\n","\n","source = MakeDataset(source_path)\n","target = MakeDataset(target_path)\n","\n","# convert sentence to word embedding\n","def tokenizer(lang):\n","  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters=\"\", split=\" \") # create tokenizer object\n","  lang_tokenizer.fit_on_texts(lang) # text need to train\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n","  return tensor, lang_tokenizer\n","\n","source_tensor, source_tokenizer = tokenizer(source)\n","target_tensor, target_tokenizer = tokenizer(target)\n","\n","# max_length sentence of input and output dataset\n","def max_length(tensor):\n","  return max(len(t) for t in tensor)\n","\n","max_length_input = max_length(source_tensor)\n","max_length_output = max_length(target_tensor)\n","\n","print(max_length_input, max_length_output)\n","\n","# # divide train and test dataset\n","# from sklearn.model_selection import train_test_split\n","# input_train, input_eval, output_train, output_eval = train_test_split(source_tensor, target_tensor, test_size=0.02)\n","# len(input_train), len(input_eval), len(output_train), len(output_eval)\n","\n","\n","# check the word embedding sentence\n","def convert(example, tokenizer):\n","  for t in example:\n","    if t != 0:\n","      print(\"%d --> %s\" % (t, tokenizer.index_word[t]))\n","\n","convert(source_tensor[0], source_tokenizer)\n","print()\n","convert(target_tensor[0], target_tokenizer)\n","\n","# prepare the dataset\n","def make_dataset(source_tensor, target_tensor, batch_size, epochs, shuffle):\n","  dataset = tf.data.Dataset.from_tensor_slices((source_tensor, target_tensor)) # divide according to the first dimension, generate dataset\n","  if shuffle:\n","    dataset = dataset.shuffle(10000)\n","    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n","  return dataset\n","\n","train_dataset = make_dataset(source_tensor, target_tensor, batch_size, epochs, True)\n","# eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)\n","print(train_dataset)\n","# print(eval_dataset)\n","\n","for x, y in train_dataset.take(1):\n","  print(x.shape)\n","  print(y.shape)\n","  print(x)\n","  print(y)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["64 62\n","3 --> <s>\n","1072 --> 1998\n","56 --> 年\n","1 --> ,\n","473 --> 经过\n","127 --> 统一\n","829 --> 部署\n","1 --> ,\n","10291 --> 伊犁州\n","1 --> ,\n","45 --> 地\n","10292 --> 两级\n","1397 --> 党委\n","287 --> 开始\n","3869 --> 尝试\n","42 --> 以\n","5381 --> 宣讲团\n","2 --> 的\n","386 --> 形式\n","1 --> ,\n","591 --> 深入\n","774 --> 学校\n","1 --> ,\n","1461 --> 村民\n","10293 --> 院落\n","1 --> ,\n","7011 --> 田间\n","7012 --> 地头\n","1 --> ,\n","95 --> 向\n","4483 --> 各族群众\n","54 --> 进行\n","7013 --> 面对面\n","3385 --> 宣讲\n","5 --> .\n","4 --> </s>\n","\n","6 --> <s>\n","9 --> in\n","922 --> 1998\n","2 --> ,\n","1 --> the\n","6184 --> yili\n","779 --> autonomous\n","3434 --> prefecture\n","245 --> cpc\n","57 --> committee\n","3 --> and\n","1 --> the\n","6184 --> yili\n","3434 --> prefecture\n","245 --> cpc\n","57 --> committee\n","76 --> made\n","1869 --> unified\n","891 --> arrangements\n","3 --> and\n","1372 --> sent\n","17 --> on\n","10 --> a\n","2046 --> trial\n","398 --> basis\n","435 --> several\n","1185 --> propaganda\n","2942 --> teams\n","903 --> deep\n","91 --> into\n","1 --> the\n","742 --> schools\n","2 --> ,\n","1946 --> villagers\n","110 --> '\n","7958 --> courtyards\n","2 --> ,\n","3 --> and\n","530 --> fields\n","8 --> to\n","333 --> carry\n","52 --> out\n","604 --> face\n","12 --> -\n","8 --> to\n","12 --> -\n","604 --> face\n","1185 --> propaganda\n","287 --> among\n","1 --> the\n","28 --> people\n","4 --> of\n","41 --> all\n","769 --> nationalities\n","5 --> .\n","7 --> </s>\n","<BatchDataset shapes: ((64, 64), (64, 62)), types: (tf.int32, tf.int32)>\n","(64, 64)\n","(64, 62)\n","tf.Tensor(\n","[[    3 10244   406 ...     0     0     0]\n"," [    3    23    74 ...     0     0     0]\n"," [    3    43  1792 ...     0     0     0]\n"," ...\n"," [    3   243     1 ...     0     0     0]\n"," [    3   203  5190 ...     0     0     0]\n"," [    3   118   372 ...     0     0     0]], shape=(64, 64), dtype=int32)\n","tf.Tensor(\n","[[  6   1  31 ...   0   0   0]\n"," [  6  27  60 ...   7   0   0]\n"," [  6 212 129 ...   0   0   0]\n"," ...\n"," [  6  96 236 ...   0   0   0]\n"," [  6 132 719 ...   0   0   0]\n"," [  6 111 426 ...   0   0   0]], shape=(64, 62), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZO42-ylK6Kj3","executionInfo":{"status":"ok","timestamp":1626786130467,"user_tz":-480,"elapsed":679,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"9f88e6e9-2030-4cd6-811e-a02b42b10fa5"},"source":["import re\n","filename = 'drive/MyDrive/en_simple.txt'\n","\n","def preprocess_sentence(s):\n","  s = s.strip()\n","  s = '<s> ' + s + ' </s>'\n","  return s\n","\n","\n","def parse_data(filename):\n","  lines = open(filename, encoding = 'UTF-8').read().strip().split('\\n')\n","  sentence_pairs = [line.split('\\t') for line in lines]\n","  preprocessed_sentence_pairs = [(preprocess_sentence(en), preprocess_sentence(cn)) for en, cn in sentence_pairs]\n","  return zip(*preprocessed_sentence_pairs)\n","\n","en_dataset, cn_dataset = parse_data(filename)\n","print(en_dataset[-1])\n","print(cn_dataset[-1])\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["<s> I mistook him for his brother . </s>\n","<s> 我 把 他 误认为 是 他 的 兄弟 。 </s>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JVyp1NOu7dHA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789751725,"user_tz":-480,"elapsed":385,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"380f7bed-c7c1-4f44-8eb7-5418d12de53d"},"source":["## encoder\n","input_vocab_size = len(source_tokenizer.word_index) + 1\n","output_vocab_size = len(target_tokenizer.word_index) + 1\n","\n","class Encoder(keras.Model):\n","    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.encoding_units = encoding_units\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","        self.gru = keras.layers.GRU(self.encoding_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)\n","        return output, state\n","  \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_size, self.encoding_units))\n","\n","encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n","\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(x, sample_hidden)\n","print(sample_output.shape, sample_hidden.shape)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["(64, 64, 1024) (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T2hlLxX_70z7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789758565,"user_tz":-480,"elapsed":453,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"af12cdc0-7064-45ad-9d30-df88d0c7f390"},"source":["## attention\n","class BahdanauAttention(keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = keras.layers.Dense(units)\n","    self.W2 = keras.layers.Dense(units)\n","    self.V = keras.layers.Dense(1)\n","\n","  def call(self, decoder_hidden, encoder_outputs):\n","    # decoder_hidden.shape: (batch_size, units)\n","    # encoder_outputs.shape: (batch_size, length, units)\n","    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n","\n","    # before V: (batch_size, length, units)\n","    # after V: (batch_size, length, 1)\n","    score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n","\n","    # shape: (batch_size, length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","    \n","    # shape: (batch_size, length, units)\n","    context_vector = attention_weights * encoder_outputs\n","\n","    # shape: (batch_size, units)\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights\n","\n","attention_model = BahdanauAttention(units = 10)\n","attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n","print(attention_results.shape, attention_weights.shape)\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["(64, 1024) (64, 64, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1b4GQky8DoC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789766367,"user_tz":-480,"elapsed":586,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"7c6049e9-3a89-4144-81f7-201a1e43e0cf"},"source":["## decoder\n","class Decoder(keras.Model):\n","  def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n","    super(Decoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.decoding_units = decoding_units\n","    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n","    self.gru = keras.layers.GRU(self.decoding_units, return_sequences= True, return_state= True, recurrent_initializer= \"glorot_uniform\")\n","    self.fc = keras.layers.Dense(vocab_size)\n","    self.attention = BahdanauAttention(self.decoding_units)\n","  \n","  def call(self, x, hidden, encoding_outputs):\n","    # context_vector.shape: (batch_size, units)\n","    context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n","\n","    # before embedding: x.shape: (batch_size, 1)\n","    # after embedding: x.shape: (batch_size, 1, embedding_units)\n","    x = self.embedding(x)\n","\n","    combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n","\n","    # output.shape: [batch_size, 1, decoding_units]\n","    # state.shape: [batch_size, decoding_units]\n","    output, state = self.gru(combined_x)\n","    \n","    # output.shape: [batch_size, decoding_units]\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","    \n","    #output.shape: [batch_size, vocab_size]\n","    output = self.fc(output)\n","\n","    return output, state, attention_weights\n","\n","decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n","outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n","\n","decoder_output, decoder_hidden, decoder_aw = outputs\n","print(decoder_output.shape, decoder_hidden.shape, decoder_aw.shape)\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["(64, 12714) (64, 1024) (64, 64, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--AXn3ojKji7","executionInfo":{"status":"ok","timestamp":1626786705402,"user_tz":-480,"elapsed":322311,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"a430c9d9-dbad-4ebf-c733-e05cb123db32"},"source":["optimizer = keras.optimizers.Adam()\n","\n","loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits= True, reduction= \"none\")\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","  mask = tf.cast(mask, dtype = loss_.dtype)\n","  loss_ += mask\n","  return tf.reduce_mean(loss_)\n","\n","@tf.function\n","def train_step(inp, targ, encoding_hidden):\n","  loss = 0\n","  with tf.GradientTape() as type:\n","    encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n","    decoding_hidden = encoding_hidden\n","\n","    for t in range(0, targ.shape[1] - 1):\n","      decoding_input = tf.expand_dims(targ[:, t], 1)\n","      predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n","      loss += loss_function(targ[:, t+1], predictions)\n","\n","    batch_loss = loss / int(targ.shape[0])\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = type.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss\n","\n","steps_per_epoch = len(source_tensor) // batch_size\n","\n","# checkpoint\n","checkpoint_dir = 'drive/MyDrive/training_checkpoints_smalldataset'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, attention_model=attention_model, decoder=decoder)\n","\n","if not os.path.exists(checkpoint_dir):\n","  os.makedirs(checkpoint_dir)\n","\n","for epoch in range(epochs):\n","  start = time.time()\n","\n","  encoding_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(inp, targ, encoding_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","\n","# saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 1.6400\n","Epoch 1 Batch 100 Loss 0.5482\n","Epoch 2 Batch 0 Loss 0.5506\n","Epoch 2 Batch 100 Loss 0.4888\n","Epoch 2 Loss 0.5139\n","Time taken for 1 epoch 15.283139944076538 sec\n","\n","Epoch 3 Batch 0 Loss 0.4857\n","Epoch 3 Batch 100 Loss 0.4829\n","Epoch 4 Batch 0 Loss 0.4146\n","Epoch 4 Batch 100 Loss 0.4223\n","Epoch 4 Loss 0.4202\n","Time taken for 1 epoch 15.510977268218994 sec\n","\n","Epoch 5 Batch 0 Loss 0.3869\n","Epoch 5 Batch 100 Loss 0.3845\n","Epoch 6 Batch 0 Loss 0.3579\n","Epoch 6 Batch 100 Loss 0.3484\n","Epoch 6 Loss 0.3462\n","Time taken for 1 epoch 15.616705179214478 sec\n","\n","Epoch 7 Batch 0 Loss 0.2928\n","Epoch 7 Batch 100 Loss 0.3072\n","Epoch 8 Batch 0 Loss 0.2729\n","Epoch 8 Batch 100 Loss 0.2942\n","Epoch 8 Loss 0.2741\n","Time taken for 1 epoch 15.292458295822144 sec\n","\n","Epoch 9 Batch 0 Loss 0.2593\n","Epoch 9 Batch 100 Loss 0.2673\n","Epoch 10 Batch 0 Loss 0.2017\n","Epoch 10 Batch 100 Loss 0.2124\n","Epoch 10 Loss 0.2130\n","Time taken for 1 epoch 15.557554960250854 sec\n","\n","Epoch 11 Batch 0 Loss 0.1860\n","Epoch 11 Batch 100 Loss 0.1827\n","Epoch 12 Batch 0 Loss 0.1549\n","Epoch 12 Batch 100 Loss 0.1715\n","Epoch 12 Loss 0.1675\n","Time taken for 1 epoch 15.391273021697998 sec\n","\n","Epoch 13 Batch 0 Loss 0.1425\n","Epoch 13 Batch 100 Loss 0.1492\n","Epoch 14 Batch 0 Loss 0.1273\n","Epoch 14 Batch 100 Loss 0.1297\n","Epoch 14 Loss 0.1386\n","Time taken for 1 epoch 21.286624431610107 sec\n","\n","Epoch 15 Batch 0 Loss 0.1233\n","Epoch 15 Batch 100 Loss 0.1242\n","Epoch 16 Batch 0 Loss 0.1247\n","Epoch 16 Batch 100 Loss 0.1221\n","Epoch 16 Loss 0.1236\n","Time taken for 1 epoch 15.373827934265137 sec\n","\n","Epoch 17 Batch 0 Loss 0.1173\n","Epoch 17 Batch 100 Loss 0.1228\n","Epoch 18 Batch 0 Loss 0.1204\n","Epoch 18 Batch 100 Loss 0.1103\n","Epoch 18 Loss 0.1179\n","Time taken for 1 epoch 15.4063241481781 sec\n","\n","Epoch 19 Batch 0 Loss 0.1138\n","Epoch 19 Batch 100 Loss 0.1194\n","Epoch 20 Batch 0 Loss 0.1112\n","Epoch 20 Batch 100 Loss 0.1093\n","Epoch 20 Loss 0.1153\n","Time taken for 1 epoch 15.36255168914795 sec\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xY7xZH_VBqdS","executionInfo":{"status":"ok","timestamp":1626788953276,"user_tz":-480,"elapsed":8468,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"e78525e3-2684-4ef4-d8e2-221a352d3487"},"source":["# evaluation model (small dataset)\n","# load model\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","encoder = Encoder(input_vocab_size, embedding_units, units, 1)\n","attention_model = BahdanauAttention(units = 10)\n","decoder = Decoder(output_vocab_size, embedding_units, units, 1)\n","optimizer = keras.optimizers.Adam()\n","checkpoint_dir = 'drive/MyDrive/training_checkpoints_smalldataset'\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, attention_model=attention_model, decoder=decoder)\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","test_filepath = 'drive/MyDrive/test_simple.txt'\n","en_dataset_target, cn_dataset_source = parse_data(test_filepath)\n","\n","def evaluation(input_sentence):\n","  input = [source_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n","\n","  input = keras.preprocessing.sequence.pad_sequences([input], maxlen= max_length_input, padding='post')\n","  input = tf.convert_to_tensor(input)\n","  # print(input.shape)\n","  result = ''\n","  encoding_hidden = encoder.initialize_hidden_state()\n","  encoding_output, encoding_hidden = encoder(input, encoding_hidden)\n","  decoding_hidden = encoding_hidden\n","  \n","  # decoding_input.shape: (1,1) --> (batch_size, length)\n","  decoding_input = tf.expand_dims([target_tokenizer.word_index['<s>']], 0)\n","    \n","  for t in range(max_length_output):\n","    predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_output)\n","  \n","    # predictions.shape: (batch_size, vocab_size)\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","    result += target_tokenizer.index_word[predicted_id] + ' '\n","  \n","    if target_tokenizer.index_word[predicted_id] == '</s>':\n","      # print(result.strip(' </s>'))\n","      return result.strip(' </s>')\n","    \n","    decoding_input = tf.expand_dims([predicted_id], 0)\n","  # print(result.strip(' </s>'))\n","  return result.strip(' </s>')\n","\n","predictions = []\n","targets = []\n","\n","for line in cn_dataset_source:\n","  result = evaluation(line)\n","  result = '<s> ' + result + ' </s>' \n","  result = result.split() \n","  predictions.append(result)\n","\n","for line in en_dataset_target:\n","  line = line.lower().split()\n","  targets.append(line)\n","\n","print(predictions)\n","print(targets)\n","## Calculate bleu score\n","score = corpus_bleu(targets, predictions)\n","print(\"Bleu Score = \" + str(100*score))\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W1.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W1.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.V.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.V.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n","[['<s>', '\\ufeffhi', '.', '</s>'], ['<s>', 'hello', '!', '</s>'], ['<s>', 'run', '.', '</s>'], ['<s>', 'wait', '!', '</s>'], ['<s>', 'hello', '!', '</s>'], ['<s>', 'i', 'try', '.', '</s>'], ['<s>', 'i', 'won', '!', '</s>'], ['<s>', 'oh', 'no', '!', '</s>'], ['<s>', 'cheers', '!', '</s>'], ['<s>', 'got', 'it', '?', '</s>'], ['<s>', 'he', 'ran', '.', '</s>'], ['<s>', 'hop', 'in', '.', '</s>'], ['<s>', 'i', 'lost', '.', '</s>'], ['<s>', 'i', 'quit', '.', '</s>'], ['<s>', \"i'm\", 'ok', '.', '</s>'], ['<s>', 'listen', '.', '</s>'], ['<s>', 'it', \"can't\", 'be', '!', '</s>'], ['<s>', 'no', 'way', '!', '</s>'], ['<s>', 'really', '?', '</s>'], ['<s>', 'try', 'some', '.', '</s>'], ['<s>', 'we', 'try', '.', '</s>'], ['<s>', 'why', 'me', '?', '</s>'], ['<s>', 'ask', 'tom', '.', '</s>'], ['<s>', 'be', 'calm', '.', '</s>'], ['<s>', 'be', 'fair', '.', '</s>'], ['<s>', 'be', 'kind', '.', '</s>'], ['<s>', 'be', 'nice', '.', '</s>'], ['<s>', 'call', 'me', '.', '</s>'], ['<s>', 'call', 'us', '.', '</s>'], ['<s>', 'come', 'in', '.', '</s>'], ['<s>', 'get', 'tom', '.', '</s>'], ['<s>', 'get', 'out', '!', '</s>'], ['<s>', 'get', 'out', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'ee', 'you', 'around', '.', '</s>'], ['<s>', 'goodbye', '!', '</s>'], ['<s>', 'hang', 'on', '!', '</s>'], ['<s>', 'he', 'came', '.', '</s>'], ['<s>', 'he', 'runs', '.', '</s>'], ['<s>', 'help', 'me', '.', '</s>'], ['<s>', 'help', 'us', '.', '</s>'], ['<s>', 'hold', 'on', '.', '</s>'], ['<s>', 'hug', 'tom', '.', '</s>'], ['<s>', 'i', 'agree', '.', '</s>'], ['<s>', \"i'm\", 'ill', '.', '</s>'], ['<s>', \"i'm\", 'old', '.', '</s>'], ['<s>', \"it's\", 'ok', '.', '</s>'], ['<s>', \"it's\", 'me', '.', '</s>'], ['<s>', 'join', 'us', '.', '</s>'], ['<s>', 'keep', 'it', '.', '</s>'], ['<s>', 'kiss', 'me', '.', '</s>'], ['<s>', 'perfect', '!', '</s>'], ['<s>', 'ee', 'you', 'around', '.', '</s>'], ['<s>', 'hut', 'up', '!', '</s>'], ['<s>', 'kip', 'it', '.', '</s>'], ['<s>', 'take', 'it', '.', '</s>'], ['<s>', 'wake', 'up', '!', '</s>'], ['<s>', 'wash', 'up', '.', '</s>'], ['<s>', 'we', 'know', '.', '</s>'], ['<s>', 'welcome', '.', '</s>'], ['<s>', 'who', 'won', '?', '</s>'], ['<s>', 'why', 'not', '?', '</s>'], ['<s>', 'you', 'run', '.', '</s>'], ['<s>', 'back', 'off', '.', '</s>'], ['<s>', 'be', 'still', '.', '</s>'], ['<s>', 'i', 'have', 'no', 'idea', '.', '</s>'], ['<s>', 'cuff', 'him', '.', '</s>'], ['<s>', 'drive', 'on', '.', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'get', 'down', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'get', 'real', '.', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'grab', 'tom', '.', '</s>'], ['<s>', 'grab', 'him', '.', '</s>'], ['<s>', 'have', 'fun', '.', '</s>'], ['<s>', 'he', 'tries', '.', '</s>'], ['<s>', 'humor', 'me', '.', '</s>'], ['<s>', 'hurry', 'up', '.', '</s>'], ['<s>', 'hurry', 'up', '.', '</s>'], ['<s>', 'i', 'forgot', '.', '</s>'], ['<s>', 'i', 'resign', '.', '</s>'], ['<s>', \"i'll\", 'pay', '.', '</s>'], ['<s>', \"i'm\", 'busy', '.', '</s>'], ['<s>', 'i', 'am', 'cold', '.', '</s>'], ['<s>', \"i'm\", 'fine', '.', '</s>'], ['<s>', \"i'm\", 'full', '.', '</s>'], ['<s>', \"i'm\", 'ill', '.', '</s>'], ['<s>', \"i'm\", 'sick', '.', '</s>'], ['<s>', \"i'm\", 'tall', '.', '</s>'], ['<s>', 'leave', 'me', '.', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', 'look', 'out', '!', '</s>'], ['<s>', 'he', 'runs', '.', '</s>'], ['<s>', 'tand', 'up', '.', '</s>'], ['<s>', 'they', 'won', '.', '</s>'], ['<s>', 'tom', 'died', '.', '</s>'], ['<s>', 'tom', 'quit', '.', '</s>'], ['<s>', 'tom', 'swam', '.', '</s>'], ['<s>', 'trust', 'me', '.', '</s>'], ['<s>', 'try', 'hard', '.', '</s>'], ['<s>', 'try', 'some', '.', '</s>'], ['<s>', 'who', 'died', '?', '</s>'], ['<s>', 'birds', 'fly', '.', '</s>'], ['<s>', 'call', 'home', '!', '</s>'], ['<s>', 'grab', 'him', '.', '</s>'], ['<s>', 'come', 'home', '.', '</s>'], ['<s>', 'do', 'it', 'now', '.', '</s>'], ['<s>', 'dogs', 'bark', '.', '</s>'], ['<s>', \"don't\", 'cry', '.', '</s>'], ['<s>', \"i'm\", 'sorry', '.', '</s>'], ['<s>', 'feel', 'this', '.', '</s>'], ['<s>', 'come', 'along', 'with', 'me', '.', '</s>'], ['<s>', 'follow', 'us', '.', '</s>'], ['<s>', 'good', 'luck', '.', '</s>'], ['<s>', 'grab', 'that', '.', '</s>'], ['<s>', 'grab', 'this', '.', '</s>'], ['<s>', 'hands', 'off', '.', '</s>'], ['<s>', \"he's\", 'lazy', '.', '</s>'], ['<s>', 'hold', 'fire', '.', '</s>'], ['<s>', 'hold', 'this', '.', '</s>'], ['<s>', 'how', 'awful', '!', '</s>'], ['<s>', \"i'm\", 'busy', '.', '</s>'], ['<s>', 'i', 'am', 'cold', '.', '</s>'], ['<s>', \"i'm\", 'ok', '.', '</s>'], ['<s>', \"i'm\", 'ill', '.', '</s>'], ['<s>', \"i'm\", 'tall', '.', '</s>'], ['<s>', 'i', 'get', 'you', '.', '</s>'], ['<s>', 'i', 'hope', 'so', '.', '</s>'], ['<s>', 'i', 'laughed', '.', '</s>'], ['<s>', 'i', 'give', 'you', 'my', 'word', '.', '</s>'], ['<s>', 'i', 'saw', 'tom', '.', '</s>'], ['<s>', \"i'll\", 'swim', '.', '</s>'], ['<s>', \"i'm\", 'a', 'man', '.', '</s>'], ['<s>', \"i'm\", 'right', '.', '</s>'], ['<s>', \"i'm\", 'sorry', '.', '</s>'], ['<s>', \"i'm\", 'sorry', '.', '</s>'], ['<s>', \"i'm\", 'young', '.', '</s>'], ['<s>', 'is', 'it', 'far', '?', '</s>'], ['<s>', 'it', 'snowed', '.', '</s>'], ['<s>', \"it's\", '3', ':', '30', '.', '</s>'], ['<s>', \"it's\", 'cold', '.', '</s>'], ['<s>', \"it's\", 'free', '.', '</s>'], ['<s>', \"it's\", 'late', '.', '</s>'], ['<s>', 'this', 'is', 'true', '.', '</s>'], ['<s>', 'let', 'me', 'in', '.', '</s>'], ['<s>', 'lie', 'still', '.', '</s>'], ['<s>', 'look', 'back', '!', '</s>'], ['<s>', 'move', 'over', '.', '</s>'], ['<s>', 'of', 'course', '!', '</s>'], ['<s>', 'of', 'course', '.', '</s>'], ['<s>', 'of', 'course', '.', '</s>'], ['<s>', 'oh', 'please', '!', '</s>'], ['<s>', 'open', 'fire', '!', '</s>'], ['<s>', 'read', 'this', '.', '</s>'], ['<s>', 'ee', 'above', '.', '</s>'], ['<s>', 'he', 'cried', '.', '</s>'], ['<s>', 'he', 'tried', '.', '</s>'], ['<s>', 'he', 'walks', '.', '</s>'], ['<s>', 'it', 'tight', '.', '</s>'], ['<s>', 'low', 'down', '.', '</s>'], ['<s>', 'tay', 'calm', '.', '</s>'], ['<s>', 'tay', 'down', '!', '</s>'], ['<s>', 'top', 'that', '!', '</s>'], ['<s>', 'take', 'care', '!', '</s>'], ['<s>', 'take', 'care', '!', '</s>'], ['<s>', 'tom', 'slept', '.', '</s>'], ['<s>', 'tom', 'swims', '.', '</s>'], ['<s>', 'tom', 'tried', '.', '</s>'], ['<s>', 'tom', 'waved', '.', '</s>'], ['<s>', \"tom'll\", 'go', '.', '</s>'], ['<s>', 'turn', 'left', '.', '</s>'], ['<s>', 'wait', 'here', '.', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'who', 'cares', '?', '</s>'], ['<s>', 'wonderful', '!', '</s>'], ['<s>', 'you', 'idiot', '!', '</s>'], ['<s>', 'you', 'stink', '.', '</s>'], ['<s>', 'all', 'aboard', '!', '</s>'], ['<s>', 'am', 'i', 'wrong', '?', '</s>'], ['<s>', 'birds', 'sing', '.', '</s>'], ['<s>', 'can', 'i', 'help', '?', '</s>'], ['<s>', 'come', 'along', '.', '</s>'], ['<s>', 'definitely', '!', '</s>'], ['<s>', \"don't\", 'move', '!', '</s>'], ['<s>', \"don't\", 'move', '.', '</s>'], ['<s>', 'fill', 'it', 'up', '.', '</s>'], ['<s>', 'follow', 'him', '.', '</s>'], ['<s>', 'god', 'exists', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'he', 'gave', 'in', '.', '</s>'], ['<s>', 'he', 'is', 'mean', '.', '</s>']]\n","[['<s>', 'hi', '.', '</s>'], ['<s>', 'hi', '.', '</s>'], ['<s>', 'run', '.', '</s>'], ['<s>', 'wait', '!', '</s>'], ['<s>', 'hello', '!', '</s>'], ['<s>', 'i', 'try', '.', '</s>'], ['<s>', 'i', 'won', '!', '</s>'], ['<s>', 'oh', 'no', '!', '</s>'], ['<s>', 'cheers', '!', '</s>'], ['<s>', 'got', 'it', '?', '</s>'], ['<s>', 'he', 'ran', '.', '</s>'], ['<s>', 'hop', 'in', '.', '</s>'], ['<s>', 'i', 'lost', '.', '</s>'], ['<s>', 'i', 'quit', '.', '</s>'], ['<s>', \"i'm\", 'ok', '.', '</s>'], ['<s>', 'listen', '.', '</s>'], ['<s>', 'no', 'way', '!', '</s>'], ['<s>', 'no', 'way', '!', '</s>'], ['<s>', 'really', '?', '</s>'], ['<s>', 'try', 'it', '.', '</s>'], ['<s>', 'we', 'try', '.', '</s>'], ['<s>', 'why', 'me', '?', '</s>'], ['<s>', 'ask', 'tom', '.', '</s>'], ['<s>', 'be', 'calm', '.', '</s>'], ['<s>', 'be', 'fair', '.', '</s>'], ['<s>', 'be', 'kind', '.', '</s>'], ['<s>', 'be', 'nice', '.', '</s>'], ['<s>', 'call', 'me', '.', '</s>'], ['<s>', 'call', 'us', '.', '</s>'], ['<s>', 'come', 'in', '.', '</s>'], ['<s>', 'get', 'tom', '.', '</s>'], ['<s>', 'get', 'out', '!', '</s>'], ['<s>', 'get', 'out', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'go', 'away', '!', '</s>'], ['<s>', 'go', 'away', '.', '</s>'], ['<s>', 'goodbye', '!', '</s>'], ['<s>', 'goodbye', '!', '</s>'], ['<s>', 'hang', 'on', '!', '</s>'], ['<s>', 'he', 'came', '.', '</s>'], ['<s>', 'he', 'runs', '.', '</s>'], ['<s>', 'help', 'me', '.', '</s>'], ['<s>', 'help', 'us', '.', '</s>'], ['<s>', 'hold', 'on', '.', '</s>'], ['<s>', 'hug', 'tom', '.', '</s>'], ['<s>', 'i', 'agree', '.', '</s>'], ['<s>', \"i'm\", 'ill', '.', '</s>'], ['<s>', \"i'm\", 'old', '.', '</s>'], ['<s>', \"it's\", 'ok', '.', '</s>'], ['<s>', \"it's\", 'me', '.', '</s>'], ['<s>', 'join', 'us', '.', '</s>'], ['<s>', 'keep', 'it', '.', '</s>'], ['<s>', 'kiss', 'me', '.', '</s>'], ['<s>', 'perfect', '!', '</s>'], ['<s>', 'see', 'you', '.', '</s>'], ['<s>', 'shut', 'up', '!', '</s>'], ['<s>', 'skip', 'it', '.', '</s>'], ['<s>', 'take', 'it', '.', '</s>'], ['<s>', 'wake', 'up', '!', '</s>'], ['<s>', 'wash', 'up', '.', '</s>'], ['<s>', 'we', 'know', '.', '</s>'], ['<s>', 'welcome', '.', '</s>'], ['<s>', 'who', 'won', '?', '</s>'], ['<s>', 'why', 'not', '?', '</s>'], ['<s>', 'you', 'run', '.', '</s>'], ['<s>', 'back', 'off', '.', '</s>'], ['<s>', 'be', 'still', '.', '</s>'], ['<s>', 'beats', 'me', '.', '</s>'], ['<s>', 'cuff', 'him', '.', '</s>'], ['<s>', 'drive', 'on', '.', '</s>'], ['<s>', 'get', 'away', '!', '</s>'], ['<s>', 'get', 'away', '!', '</s>'], ['<s>', 'get', 'down', '!', '</s>'], ['<s>', 'get', 'lost', '!', '</s>'], ['<s>', 'get', 'real', '.', '</s>'], ['<s>', 'good', 'job', '!', '</s>'], ['<s>', 'good', 'job', '!', '</s>'], ['<s>', 'grab', 'tom', '.', '</s>'], ['<s>', 'grab', 'him', '.', '</s>'], ['<s>', 'have', 'fun', '.', '</s>'], ['<s>', 'he', 'tries', '.', '</s>'], ['<s>', 'humor', 'me', '.', '</s>'], ['<s>', 'hurry', 'up', '.', '</s>'], ['<s>', 'hurry', 'up', '.', '</s>'], ['<s>', 'i', 'forgot', '.', '</s>'], ['<s>', 'i', 'resign', '.', '</s>'], ['<s>', \"i'll\", 'pay', '.', '</s>'], ['<s>', \"i'm\", 'busy', '.', '</s>'], ['<s>', \"i'm\", 'cold', '.', '</s>'], ['<s>', \"i'm\", 'fine', '.', '</s>'], ['<s>', \"i'm\", 'full', '.', '</s>'], ['<s>', \"i'm\", 'sick', '.', '</s>'], ['<s>', \"i'm\", 'sick', '.', '</s>'], ['<s>', \"i'm\", 'tall', '.', '</s>'], ['<s>', 'leave', 'me', '.', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', \"let's\", 'go', '!', '</s>'], ['<s>', 'look', 'out', '!', '</s>'], ['<s>', 'she', 'runs', '.', '</s>'], ['<s>', 'stand', 'up', '.', '</s>'], ['<s>', 'they', 'won', '.', '</s>'], ['<s>', 'tom', 'died', '.', '</s>'], ['<s>', 'tom', 'quit', '.', '</s>'], ['<s>', 'tom', 'swam', '.', '</s>'], ['<s>', 'trust', 'me', '.', '</s>'], ['<s>', 'try', 'hard', '.', '</s>'], ['<s>', 'try', 'some', '.', '</s>'], ['<s>', 'who', 'died', '?', '</s>'], ['<s>', 'birds', 'fly', '.', '</s>'], ['<s>', 'call', 'home', '!', '</s>'], ['<s>', 'catch', 'him', '.', '</s>'], ['<s>', 'come', 'home', '.', '</s>'], ['<s>', 'do', 'it', 'now', '.', '</s>'], ['<s>', 'dogs', 'bark', '.', '</s>'], ['<s>', \"don't\", 'cry', '.', '</s>'], ['<s>', 'excuse', 'me', '.', '</s>'], ['<s>', 'feel', 'this', '.', '</s>'], ['<s>', 'follow', 'me', '.', '</s>'], ['<s>', 'follow', 'us', '.', '</s>'], ['<s>', 'good', 'luck', '.', '</s>'], ['<s>', 'grab', 'that', '.', '</s>'], ['<s>', 'grab', 'this', '.', '</s>'], ['<s>', 'hands', 'off', '.', '</s>'], ['<s>', \"he's\", 'lazy', '.', '</s>'], ['<s>', 'hold', 'fire', '.', '</s>'], ['<s>', 'hold', 'this', '.', '</s>'], ['<s>', 'how', 'awful', '!', '</s>'], ['<s>', 'i', 'am', 'busy', '.', '</s>'], ['<s>', 'i', 'am', 'cold', '.', '</s>'], ['<s>', 'i', 'am', 'okay', '.', '</s>'], ['<s>', 'i', 'am', 'sick', '.', '</s>'], ['<s>', 'i', 'am', 'tall', '.', '</s>'], ['<s>', 'i', 'get', 'you', '.', '</s>'], ['<s>', 'i', 'hope', 'so', '.', '</s>'], ['<s>', 'i', 'laughed', '.', '</s>'], ['<s>', 'i', 'promise', '.', '</s>'], ['<s>', 'i', 'saw', 'tom', '.', '</s>'], ['<s>', \"i'll\", 'swim', '.', '</s>'], ['<s>', \"i'm\", 'a', 'man', '.', '</s>'], ['<s>', \"i'm\", 'right', '.', '</s>'], ['<s>', \"i'm\", 'sorry', '.', '</s>'], ['<s>', \"i'm\", 'sorry', '.', '</s>'], ['<s>', \"i'm\", 'young', '.', '</s>'], ['<s>', 'is', 'it', 'far', '?', '</s>'], ['<s>', 'it', 'snowed', '.', '</s>'], ['<s>', \"it's\", '3', ':', '30', '.', '</s>'], ['<s>', \"it's\", 'cold', '.', '</s>'], ['<s>', \"it's\", 'free', '.', '</s>'], ['<s>', \"it's\", 'late', '.', '</s>'], ['<s>', \"it's\", 'true', '.', '</s>'], ['<s>', 'let', 'me', 'in', '.', '</s>'], ['<s>', 'lie', 'still', '.', '</s>'], ['<s>', 'look', 'back', '!', '</s>'], ['<s>', 'move', 'over', '.', '</s>'], ['<s>', 'of', 'course', '!', '</s>'], ['<s>', 'of', 'course', '.', '</s>'], ['<s>', 'of', 'course', '.', '</s>'], ['<s>', 'oh', 'please', '!', '</s>'], ['<s>', 'open', 'fire', '!', '</s>'], ['<s>', 'read', 'this', '.', '</s>'], ['<s>', 'see', 'above', '.', '</s>'], ['<s>', 'she', 'cried', '.', '</s>'], ['<s>', 'she', 'tried', '.', '</s>'], ['<s>', 'she', 'walks', '.', '</s>'], ['<s>', 'sit', 'tight', '.', '</s>'], ['<s>', 'slow', 'down', '.', '</s>'], ['<s>', 'stay', 'calm', '.', '</s>'], ['<s>', 'stay', 'down', '!', '</s>'], ['<s>', 'stop', 'that', '!', '</s>'], ['<s>', 'take', 'care', '!', '</s>'], ['<s>', 'take', 'care', '.', '</s>'], ['<s>', 'tom', 'slept', '.', '</s>'], ['<s>', 'tom', 'swims', '.', '</s>'], ['<s>', 'tom', 'tried', '.', '</s>'], ['<s>', 'tom', 'waved', '.', '</s>'], ['<s>', \"tom'll\", 'go', '.', '</s>'], ['<s>', 'turn', 'left', '.', '</s>'], ['<s>', 'wait', 'here', '.', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'well', 'done', '!', '</s>'], ['<s>', 'who', 'cares', '?', '</s>'], ['<s>', 'wonderful', '!', '</s>'], ['<s>', 'you', 'idiot', '!', '</s>'], ['<s>', 'you', 'stink', '.', '</s>'], ['<s>', 'all', 'aboard', '!', '</s>'], ['<s>', 'am', 'i', 'wrong', '?', '</s>'], ['<s>', 'birds', 'sing', '.', '</s>'], ['<s>', 'can', 'i', 'help', '?', '</s>'], ['<s>', 'come', 'along', '.', '</s>'], ['<s>', 'definitely', '!', '</s>'], ['<s>', \"don't\", 'move', '!', '</s>'], ['<s>', \"don't\", 'move', '.', '</s>'], ['<s>', 'fill', 'it', 'up', '.', '</s>'], ['<s>', 'follow', 'him', '.', '</s>'], ['<s>', 'god', 'exists', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'he', 'gave', 'in', '.', '</s>'], ['<s>', 'he', 'is', 'mean', '.', '</s>']]\n","Bleu Score = 67.86262455654717\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHs7N9W6Pyx8","executionInfo":{"status":"ok","timestamp":1626789948479,"user_tz":-480,"elapsed":80400,"user":{"displayName":"Ruochen Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHx3p37j1Sc0fTrmsFPIP8eISSnVh9gveM0EcM=s64","userId":"17861941706632682547"}},"outputId":"2b681a75-7da6-4077-f970-39c6f412eb4b"},"source":["# evaluation model\n","# load model\n","import jieba\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","encoder = Encoder(input_vocab_size, embedding_units, units, 1)\n","attention_model = BahdanauAttention(units = 10)\n","decoder = Decoder(output_vocab_size, embedding_units, units, 1)\n","optimizer = keras.optimizers.Adam()\n","checkpoint_dir = 'drive/MyDrive/training_checkpoints'\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, attention_model=attention_model, decoder=decoder)\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","test_input_filepath = 'drive/MyDrive/Corpus/test_chinese.txt'\n","test_output_filepath = 'drive/MyDrive/Corpus/test_english.txt'\n","\n","def seg_depart(input_sentence):\n","  # Chinese word segmentation for each line in the document\n","  sentence_depart = jieba.cut(input_sentence.strip())\n","  outstr = ''\n","  for word in sentence_depart:\n","      if word != '\\n':\n","          outstr += word\n","          outstr += \" \"\n","  return outstr\n","\n","def evaluation(input_sentence):\n","  # attention_matrix = np.zeros((max_length_output, max_length_input))\n","  input_sentence = seg_depart(input_sentence)\n","  input_sentence = \"<s> \" + input_sentence + \"</s>\"\n","  \n","  input = [source_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n","\n","  input = keras.preprocessing.sequence.pad_sequences([input], maxlen= max_length_input, padding='post')\n","  input = tf.convert_to_tensor(input)\n","  # print(input.shape)\n","  result = ''\n","  encoding_hidden = encoder.initialize_hidden_state()\n","  encoding_output, encoding_hidden = encoder(input, encoding_hidden)\n","  decoding_hidden = encoding_hidden\n","  \n","  # decoding_input.shape: (1,1) --> (batch_size, length)\n","  decoding_input = tf.expand_dims([target_tokenizer.word_index['<s>']], 0)\n","    \n","  for t in range(max_length_output):\n","    predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_output)\n","  \n","    # predictions.shape: (batch_size, vocab_size)\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","    result += target_tokenizer.index_word[predicted_id] + ' '\n","  \n","    if target_tokenizer.index_word[predicted_id] == '</s>':\n","      # print(result.strip(' </s>'))\n","      return result.strip(' </s>')\n","    \n","    decoding_input = tf.expand_dims([predicted_id], 0)\n","  # print(result.strip(' </s>'))\n","  return result.strip(' </s>')\n","\n","predictions = []\n","targets = []\n","\n","for line in codecs.open(test_input_filepath, 'r', 'utf-8').read().splitlines():\n","  result = evaluation(line)\n","  result = result.split()\n","  predictions.append(result)\n","\n","for line in codecs.open(test_output_filepath, 'r', 'utf-8').read().splitlines():\n","  line = line.lower().split()\n","  targets.append([line])\n","\n","## Calculate bleu score\n","score = corpus_bleu(targets, predictions)\n","print(\"Bleu Score = \" + str(100*score))\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W1.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W1.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.W2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.V.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).attention_model.V.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"],"name":"stdout"},{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.822 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"},{"output_type":"stream","text":["Bleu Score = 7.573818431593465\n"],"name":"stdout"}]}]}